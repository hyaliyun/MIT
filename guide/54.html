<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MIT</title>
    <meta name="description" content="MIT">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/assets/style.BySnLh8m.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.7uGsmnbX.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.QB-wH4Se.js">
    <link rel="modulepreload" href="/assets/chunks/framework.B1z0IdBH.js">
    <link rel="modulepreload" href="/assets/guide_54.md.DjitNciB.lean.js">
    <link rel="icon" href="/logo.svg">
    <link rel="icon" type="image/svg+xml" href="/images/logo.png">
    <meta name="theme-color" content="#3c8772">
    <meta property="og:url" content="https://www.252x.com">
    <meta property="og:type" content="website">
    <meta property="og:title" content="MIT">
    <meta property="og:description" content="MIT">
    <meta property="og:image" content="/images/logo.png">
    <meta name="twitter:site" content="@book">
    <meta name="twitter:card" content="summary">
    <link rel="preconnect" href="https://f.543x.com">
    <script>(()=>{const e=(o,r,t=!1)=>{const s=localStorage.getItem(o);(s?s!=="false":t)&&document.documentElement.classList.add(r)};e("vue-docs-prefer-composition","prefer-composition",!0),e("vue-docs-prefer-sfc","prefer-sfc",!0),window.__VUE_BANNER_ID__="vt2024_1",e(`vue-docs-banner-${__VUE_BANNER_ID__}`,"banner-dismissed")})();</script>
    <script>location.search.includes("?uwu")&&document.documentElement.classList.add("uwu");</script>
    <script src="https://cdn.usefathom.com/script.js" data-site="XNOLWPLB" data-spa="auto" defer></script>
    <script src="https://vueschool.io/banner.js?affiliate=book&amp;type=top" async></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
  </head>
  <body>
    <div id="app"><div class="VPApp" data-v-e4982c5a><!--[--><span tabindex="-1" data-v-ebeb79d9></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-ebeb79d9>Skip to content</a><!--]--><!----><!--[--><div class="banner" data-v-8f28d446><p class="vt-banner-text" data-v-8f28d446><span class="vt-text-primary" data-v-8f28d446>VueConf Toronto</span><span class="vt-tagline" data-v-8f28d446> - Join the premier TypeScript conference</span><a target="_blank" class="vt-primary-action" href="https://www.543x.com" data-v-8f28d446> Register </a></p><button data-v-8f28d446><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="close" data-v-8f28d446><path d="M18.9,10.9h-6v-6c0-0.6-0.4-1-1-1s-1,0.4-1,1v6h-6c-0.6,0-1,0.4-1,1s0.4,1,1,1h6v6c0,0.6,0.4,1,1,1s1-0.4,1-1v-6h6c0.6,0,1-0.4,1-1S19.5,10.9,18.9,10.9z"></path></svg></button><p class="vt-banner-text vt-coupon" data-v-8f28d446><span class="vt-text-primary" data-v-8f28d446>www</span> 543x <span class="vt-text-primary" data-v-8f28d446>.com</span></p></div><!--]--><header class="VPNav nav-bar stick" data-v-e4982c5a data-v-9cbed0dc><div class="VPNavBar" data-v-9cbed0dc data-v-78ea45ed><div class="container" data-v-78ea45ed><a class="VPNavBarTitle" href="/" data-v-78ea45ed data-v-4b84c549><!--[--><svg class="logo" viewBox="0 0 128 128" width="24" height="24" data-v-4b84c549><path fill="#42b883" d="M78.8,10L64,35.4L49.2,10H0l64,110l64-110C128,10,78.8,10,78.8,10z" data-v-4b84c549></path><path fill="#35495e" d="M78.8,10L64,35.4L49.2,10H25.6L64,76l38.4-66H78.8z" data-v-4b84c549></path></svg><span class="text" data-v-4b84c549>Vue.js</span><!--]--></a><div class="content" data-v-78ea45ed><div class="VPNavBarSearch search" data-v-78ea45ed><!----><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><!----></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-78ea45ed data-v-2cfd1945><span id="main-nav-aria-label" class="visually-hidden" data-v-2cfd1945>Main Navigation</span><!--[--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">data <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/drive/1.html"><!--[-->data1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/2.html"><!--[-->data2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/3.html"><!--[-->data3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/4.html"><!--[-->data4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/5.html"><!--[-->data5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/6.html"><!--[-->data6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/7.html"><!--[-->data7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/8.html"><!--[-->data8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/9.html"><!--[-->data9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/10.html"><!--[-->data10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/11.html"><!--[-->data11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/12.html"><!--[-->data12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/13.html"><!--[-->data13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/14.html"><!--[-->data14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/15.html"><!--[-->data15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/16.html"><!--[-->data16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/17.html"><!--[-->data17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/18.html"><!--[-->data18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/19.html"><!--[-->data19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/20.html"><!--[-->data20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/21.html"><!--[-->data21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/22.html"><!--[-->data22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/23.html"><!--[-->data23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/24.html"><!--[-->data24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/25.html"><!--[-->data25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/26.html"><!--[-->data26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/27.html"><!--[-->data27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/28.html"><!--[-->data28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/29.html"><!--[-->data29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/30.html"><!--[-->data30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/31.html"><!--[-->data31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/32.html"><!--[-->data32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/33.html"><!--[-->data33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/34.html"><!--[-->data34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/35.html"><!--[-->data35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/36.html"><!--[-->data36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/37.html"><!--[-->data37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/38.html"><!--[-->data38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/39.html"><!--[-->data39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/40.html"><!--[-->data40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/41.html"><!--[-->data41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/42.html"><!--[-->data42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/43.html"><!--[-->data43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/44.html"><!--[-->data44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/45.html"><!--[-->data45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/46.html"><!--[-->data46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/47.html"><!--[-->data47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/48.html"><!--[-->data48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/49.html"><!--[-->data49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/50.html"><!--[-->data50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/51.html"><!--[-->data51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/52.html"><!--[-->data52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/53.html"><!--[-->data53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/54.html"><!--[-->data54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/55.html"><!--[-->data55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/56.html"><!--[-->data56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/57.html"><!--[-->data57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/58.html"><!--[-->data58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/59.html"><!--[-->data59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/60.html"><!--[-->data60<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">grok <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/grok/1.html"><!--[-->grok1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/2.html"><!--[-->grok2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/3.html"><!--[-->grok3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/4.html"><!--[-->grok4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/5.html"><!--[-->grok5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/6.html"><!--[-->grok6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/7.html"><!--[-->grok7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/8.html"><!--[-->grok8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/9.html"><!--[-->grok9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/10.html"><!--[-->grok10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/11.html"><!--[-->grok11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/12.html"><!--[-->grok12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/13.html"><!--[-->grok13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/14.html"><!--[-->grok14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/15.html"><!--[-->grok15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/16.html"><!--[-->grok16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/17.html"><!--[-->grok17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/18.html"><!--[-->grok18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/19.html"><!--[-->grok19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/20.html"><!--[-->grok20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/21.html"><!--[-->grok21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/22.html"><!--[-->grok22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/23.html"><!--[-->grok23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/24.html"><!--[-->grok24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/25.html"><!--[-->grok25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/26.html"><!--[-->grok26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/27.html"><!--[-->grok27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/28.html"><!--[-->grok28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/29.html"><!--[-->grok29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/30.html"><!--[-->grok30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/31.html"><!--[-->grok31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/32.html"><!--[-->grok32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/33.html"><!--[-->grok33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/34.html"><!--[-->grok34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/35.html"><!--[-->grok35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/36.html"><!--[-->grok36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/37.html"><!--[-->grok37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/38.html"><!--[-->grok38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/39.html"><!--[-->grok39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/40.html"><!--[-->grok40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/41.html"><!--[-->grok41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/42.html"><!--[-->grok42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/43.html"><!--[-->grok43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/44.html"><!--[-->grok44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/45.html"><!--[-->grok45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/46.html"><!--[-->grok46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/47.html"><!--[-->grok47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/48.html"><!--[-->grok48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/49.html"><!--[-->grok49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/50.html"><!--[-->grok50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/51.html"><!--[-->grok51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/52.html"><!--[-->grok52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/53.html"><!--[-->grok53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/54.html"><!--[-->grok54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/55.html"><!--[-->grok55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/56.html"><!--[-->grok56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/57.html"><!--[-->grok57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/58.html"><!--[-->grok58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/59.html"><!--[-->grok59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/60.html"><!--[-->grok60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/61.html"><!--[-->grok61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/62.html"><!--[-->grok62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/63.html"><!--[-->grok63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/64.html"><!--[-->grok64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/65.html"><!--[-->grok65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/66.html"><!--[-->grok66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/67.html"><!--[-->grok67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/68.html"><!--[-->grok68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup active" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">wiki <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/guide/1.html"><!--[-->wiki1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/2.html"><!--[-->wiki2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/3.html"><!--[-->wiki3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/4.html"><!--[-->wiki4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/5.html"><!--[-->wiki5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/6.html"><!--[-->wiki6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/7.html"><!--[-->wiki7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/8.html"><!--[-->wiki8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/9.html"><!--[-->wiki9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/10.html"><!--[-->wiki10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/11.html"><!--[-->wiki11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/12.html"><!--[-->wiki12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/13.html"><!--[-->wiki13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/14.html"><!--[-->wiki14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/15.html"><!--[-->wiki15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/16.html"><!--[-->wiki16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/17.html"><!--[-->wiki17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/18.html"><!--[-->wiki18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/19.html"><!--[-->wiki19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/20.html"><!--[-->wiki20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/21.html"><!--[-->wiki21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/22.html"><!--[-->wiki22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/23.html"><!--[-->wiki23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/24.html"><!--[-->wiki24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/25.html"><!--[-->wiki25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/26.html"><!--[-->wiki26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/27.html"><!--[-->wiki27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/28.html"><!--[-->wiki28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/29.html"><!--[-->wiki29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/30.html"><!--[-->wiki30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/31.html"><!--[-->wiki31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/32.html"><!--[-->wiki32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/33.html"><!--[-->wiki33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/34.html"><!--[-->wiki34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/35.html"><!--[-->wiki35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/36.html"><!--[-->wiki36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/37.html"><!--[-->wiki37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/38.html"><!--[-->wiki38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/39.html"><!--[-->wiki39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/40.html"><!--[-->wiki40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/41.html"><!--[-->wiki41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/42.html"><!--[-->wiki42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/43.html"><!--[-->wiki43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/44.html"><!--[-->wiki44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/45.html"><!--[-->wiki45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/46.html"><!--[-->wiki46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/47.html"><!--[-->wiki47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/48.html"><!--[-->wiki48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/49.html"><!--[-->wiki49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/50.html"><!--[-->wiki50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/51.html"><!--[-->wiki51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/52.html"><!--[-->wiki52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/53.html"><!--[-->wiki53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/54.html"><!--[-->wiki54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/55.html"><!--[-->wiki55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/56.html"><!--[-->wiki56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/57.html"><!--[-->wiki57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/58.html"><!--[-->wiki58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/59.html"><!--[-->wiki59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/60.html"><!--[-->wiki60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/61.html"><!--[-->wiki61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/62.html"><!--[-->wiki62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/63.html"><!--[-->wiki63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/64.html"><!--[-->wiki64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/65.html"><!--[-->wiki65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/66.html"><!--[-->wiki66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/67.html"><!--[-->wiki67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/68.html"><!--[-->wiki68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">deep <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/1.html"><!--[-->deep1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/2.html"><!--[-->deep2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/3.html"><!--[-->deep3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/4.html"><!--[-->deep4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/5.html"><!--[-->deep5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/6.html"><!--[-->deep6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/7.html"><!--[-->deep7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/8.html"><!--[-->deep8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/9.html"><!--[-->deep9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/10.html"><!--[-->deep10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/11.html"><!--[-->deep11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/12.html"><!--[-->deep12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/13.html"><!--[-->deep13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/14.html"><!--[-->deep14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/15.html"><!--[-->deep15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/16.html"><!--[-->deep16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/17.html"><!--[-->deep17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/18.html"><!--[-->deep18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/19.html"><!--[-->deep19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/20.html"><!--[-->deep20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/21.html"><!--[-->deep21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/22.html"><!--[-->deep22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/23.html"><!--[-->deep23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/24.html"><!--[-->deep24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/25.html"><!--[-->deep25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/26.html"><!--[-->deep26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/27.html"><!--[-->deep27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/28.html"><!--[-->deep28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/29.html"><!--[-->deep29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/30.html"><!--[-->deep30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/31.html"><!--[-->deep31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/32.html"><!--[-->deep32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/33.html"><!--[-->deep33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/34.html"><!--[-->deep34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/35.html"><!--[-->deep35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/36.html"><!--[-->deep36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/37.html"><!--[-->deep37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/38.html"><!--[-->deep38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/39.html"><!--[-->deep39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/40.html"><!--[-->deep40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/41.html"><!--[-->deep41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/42.html"><!--[-->deep42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/43.html"><!--[-->deep43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/44.html"><!--[-->deep44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/45.html"><!--[-->deep45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/46.html"><!--[-->deep46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/47.html"><!--[-->deep47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/48.html"><!--[-->deep48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/49.html"><!--[-->deep49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/50.html"><!--[-->deep50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/51.html"><!--[-->deep51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/52.html"><!--[-->deep52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/53.html"><!--[-->deep53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/54.html"><!--[-->deep54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/55.html"><!--[-->deep55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/56.html"><!--[-->deep56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/57.html"><!--[-->deep57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/58.html"><!--[-->deep58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/59.html"><!--[-->deep59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/60.html"><!--[-->deep60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/61.html"><!--[-->deep61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/62.html"><!--[-->deep62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/63.html"><!--[-->deep63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/64.html"><!--[-->deep64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/65.html"><!--[-->deep65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/66.html"><!--[-->deep66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/67.html"><!--[-->deep67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/68.html"><!--[-->deep68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">quotes <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/1.html"><!--[-->quotes1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/2.html"><!--[-->quotes2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/3.html"><!--[-->quotes3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/4.html"><!--[-->quotes4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/5.html"><!--[-->quotes5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/6.html"><!--[-->quotes6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/7.html"><!--[-->quotes7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/8.html"><!--[-->quotes8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/9.html"><!--[-->quotes9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/10.html"><!--[-->quotes10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/11.html"><!--[-->quotes11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/12.html"><!--[-->quotes12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/13.html"><!--[-->quotes13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/14.html"><!--[-->quotes14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/15.html"><!--[-->quotes15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/16.html"><!--[-->quotes16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/17.html"><!--[-->quotes17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/18.html"><!--[-->quotes18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/19.html"><!--[-->quotes19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/20.html"><!--[-->quotes20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/21.html"><!--[-->quotes21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/22.html"><!--[-->quotes22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/23.html"><!--[-->quotes23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/24.html"><!--[-->quotes24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/25.html"><!--[-->quotes25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/26.html"><!--[-->quotes26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/27.html"><!--[-->quotes27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/28.html"><!--[-->quotes28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/29.html"><!--[-->quotes29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/30.html"><!--[-->quotes30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/31.html"><!--[-->quotes31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/32.html"><!--[-->quotes32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/33.html"><!--[-->quotes33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/34.html"><!--[-->quotes34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/35.html"><!--[-->quotes35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/36.html"><!--[-->quotes36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/37.html"><!--[-->quotes37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/38.html"><!--[-->quotes38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/39.html"><!--[-->quotes39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/40.html"><!--[-->quotes40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/41.html"><!--[-->quotes41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/42.html"><!--[-->quotes42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/43.html"><!--[-->quotes43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/44.html"><!--[-->quotes44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/45.html"><!--[-->quotes45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/46.html"><!--[-->quotes46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/47.html"><!--[-->quotes47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/48.html"><!--[-->quotes48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/49.html"><!--[-->quotes49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/50.html"><!--[-->quotes50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/51.html"><!--[-->quotes51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/52.html"><!--[-->quotes52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/53.html"><!--[-->quotes53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/54.html"><!--[-->quotes54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/55.html"><!--[-->quotes55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/56.html"><!--[-->quotes56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/57.html"><!--[-->quotes57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/58.html"><!--[-->quotes58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/59.html"><!--[-->quotes59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/60.html"><!--[-->quotes60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/61.html"><!--[-->quotes61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/62.html"><!--[-->quotes62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/63.html"><!--[-->quotes63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/64.html"><!--[-->quotes64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/65.html"><!--[-->quotes65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/66.html"><!--[-->quotes66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/67.html"><!--[-->quotes67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/68.html"><!--[-->quotes68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">chatai <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/1.html"><!--[-->chatai1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/2.html"><!--[-->chatai2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/3.html"><!--[-->chatai3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/4.html"><!--[-->chatai4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/5.html"><!--[-->chatai5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/6.html"><!--[-->chatai6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/7.html"><!--[-->chatai7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/8.html"><!--[-->chatai8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/9.html"><!--[-->chatai9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/10.html"><!--[-->chatai10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/11.html"><!--[-->chatai11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/12.html"><!--[-->chatai12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/13.html"><!--[-->chatai13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/14.html"><!--[-->chatai14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/15.html"><!--[-->chatai15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/16.html"><!--[-->chatai16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/17.html"><!--[-->chatai17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/18.html"><!--[-->chatai18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/19.html"><!--[-->chatai19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/20.html"><!--[-->chatai20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/21.html"><!--[-->chatai21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/22.html"><!--[-->chatai22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/23.html"><!--[-->chatai23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/24.html"><!--[-->chatai24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/25.html"><!--[-->chatai25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/26.html"><!--[-->chatai26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/27.html"><!--[-->chatai27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/28.html"><!--[-->chatai28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/29.html"><!--[-->chatai29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/30.html"><!--[-->chatai30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/31.html"><!--[-->chatai31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/32.html"><!--[-->chatai32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/33.html"><!--[-->chatai33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/34.html"><!--[-->chatai34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/35.html"><!--[-->chatai35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/36.html"><!--[-->chatai36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/37.html"><!--[-->chatai37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/38.html"><!--[-->chatai38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/39.html"><!--[-->chatai39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/40.html"><!--[-->chatai40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/41.html"><!--[-->chatai41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/42.html"><!--[-->chatai42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/43.html"><!--[-->chatai43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/44.html"><!--[-->chatai44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/45.html"><!--[-->chatai45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/46.html"><!--[-->chatai46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/47.html"><!--[-->chatai47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/48.html"><!--[-->chatai48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/49.html"><!--[-->chatai49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/50.html"><!--[-->chatai50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/51.html"><!--[-->chatai51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/52.html"><!--[-->chatai52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/53.html"><!--[-->chatai53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/54.html"><!--[-->chatai54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/55.html"><!--[-->chatai55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/56.html"><!--[-->chatai56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/57.html"><!--[-->chatai57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/58.html"><!--[-->chatai58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/59.html"><!--[-->chatai59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/60.html"><!--[-->chatai60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/61.html"><!--[-->chatai61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/62.html"><!--[-->chatai62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/63.html"><!--[-->chatai63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/64.html"><!--[-->chatai64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/65.html"><!--[-->chatai65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/66.html"><!--[-->chatai66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/67.html"><!--[-->chatai67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/68.html"><!--[-->chatai68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">library <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/library/1.html"><!--[-->library1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/2.html"><!--[-->library2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/3.html"><!--[-->library3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/4.html"><!--[-->library4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/5.html"><!--[-->library5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/6.html"><!--[-->library6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/7.html"><!--[-->library7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/8.html"><!--[-->library8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/9.html"><!--[-->library9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/10.html"><!--[-->library10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/11.html"><!--[-->library11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/12.html"><!--[-->library12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/13.html"><!--[-->library13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/14.html"><!--[-->library14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/15.html"><!--[-->library15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/16.html"><!--[-->library16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/17.html"><!--[-->library17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/18.html"><!--[-->library18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/19.html"><!--[-->library19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/20.html"><!--[-->library20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/21.html"><!--[-->library21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/22.html"><!--[-->library22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/23.html"><!--[-->library23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/24.html"><!--[-->library24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/25.html"><!--[-->library25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/26.html"><!--[-->library26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/27.html"><!--[-->library27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/28.html"><!--[-->library28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/29.html"><!--[-->library29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/30.html"><!--[-->library30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/31.html"><!--[-->library31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/32.html"><!--[-->library32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/33.html"><!--[-->library33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/34.html"><!--[-->library34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/35.html"><!--[-->library35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/36.html"><!--[-->library36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/37.html"><!--[-->library37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/38.html"><!--[-->library38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/39.html"><!--[-->library39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/40.html"><!--[-->library40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/41.html"><!--[-->library41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/42.html"><!--[-->library42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/43.html"><!--[-->library43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/44.html"><!--[-->library44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/45.html"><!--[-->library45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/46.html"><!--[-->library46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/47.html"><!--[-->library47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/48.html"><!--[-->library48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/49.html"><!--[-->library49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/50.html"><!--[-->library50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/51.html"><!--[-->library51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/52.html"><!--[-->library52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/53.html"><!--[-->library53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/54.html"><!--[-->library54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/55.html"><!--[-->library55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/56.html"><!--[-->library56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/57.html"><!--[-->library57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/58.html"><!--[-->library58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/59.html"><!--[-->library59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/60.html"><!--[-->library60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/61.html"><!--[-->library61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/62.html"><!--[-->library62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/63.html"><!--[-->library63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/64.html"><!--[-->library64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/65.html"><!--[-->library65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/66.html"><!--[-->library66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/67.html"><!--[-->library67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/68.html"><!--[-->library68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">ecosystem <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">website</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/partners/"><!--[-->partners<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/themes.html"><!--[-->website<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/newsletters.html"><!--[-->deepseekletters<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/navigation.html"><!--[-->AI Navigation<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/DeepSeek.html"><!--[-->DeepSeek-V3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.474x.com" target="_blank" rel="noopener noreferrer"><!--[-->474x.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.494x.com" target="_blank" rel="noopener noreferrer"><!--[-->494x.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.64ii.com" target="_blank" rel="noopener noreferrer"><!--[-->64ii.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.81oo.com" target="_blank" rel="noopener noreferrer"><!--[-->81oo.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">Library</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://e.m44m.com/" target="_blank" rel="noopener noreferrer"><!--[-->Vue Router<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://f.m44m.com/" target="_blank" rel="noopener noreferrer"><!--[-->Pinia<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.82ii.com" target="_blank" rel="noopener noreferrer"><!--[-->tool<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">Vue</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://g.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Vue Mastery<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://h.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Vue School<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">help</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://i.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Discord<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://github.com/hyaliyun/MIT" target="_blank" rel="noopener noreferrer"><!--[-->GitHub<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->DEV<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">MIT</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://c.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->blog<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://d.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Twitter<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://e.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Activity<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://w.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->CMS<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->deepseekmagSheets<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.434x.com" target="_blank" rel="noopener noreferrer"><!--[-->Tailwind<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.434x.com" target="_blank" rel="noopener noreferrer"><!--[-->Three.js<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->youtube<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/about/team.html" data-v-2cfd1945 data-v-c3f7059f><!--[-->team<!--]--><!----><!----></a><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup active" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">show <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/drive/donation.html"><!--[-->donation<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/PromptLibrary.html"><!--[-->PromptLibrary<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/prompt.html"><!--[-->prompt<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/team.html"><!--[-->crypto<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/partners/"><!--[-->partners<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.3kk3.com" target="_blank" rel="noopener noreferrer"><!--[-->3kk3.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseek<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://c.4s5s.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr1<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.6n7n.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr2<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://f.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr3<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://c.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr4<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.l00m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr5<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://g.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr6<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/swap/app.html" data-v-2cfd1945 data-v-c3f7059f><!--[-->swap<!--]--><!----><!----></a><!--]--><!--]--><div class="vt-flyout VPNavBarMenuGroup active VPNavBarLocale" data-v-2cfd1945 data-v-802bec0f><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="Select Language"><!--[--><div class="vt-locales-btn-icon-container" data-v-802bec0f><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-locales-btn-icon" data-v-802bec0f><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg></div><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><!--[--><div class="vt-menu-items x-padding" data-v-802bec0f><!--[--><div class="vt-locales-menu-item"><a href="https://g.m44m.com/guide/54.html" target="_blank" class="vt-locales-menu-item-text"> <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><a href="https://github.com/hyaliyun/MIT" title=" Repository" target="_blank" class="vt-locales-btn-icon-container"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-locales-btn-icon repo"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a></div><!--]--></div><!----><!--]--><!--]--><!--]--></div></div></div></nav><div class="VPNavBarAppearance appearance" data-v-78ea45ed data-v-7e4f86a7><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-7e4f86a7><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="vt-social-links VPNavBarSocialLinks social-links" data-v-78ea45ed data-v-44bed5da><!--[--><a class="vt-social-link is-small" href="https://github.com/hyaliyun/MIT/" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div><div class="vt-flyout VPNavBarExtra extra" data-v-78ea45ed data-v-d9c85796><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-icon"><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><div class="vt-menu-group" data-v-d9c85796><div class="vt-menu-item item" data-v-d9c85796><p class="vt-menu-label" data-v-d9c85796>Appearance</p><div class="vt-menu-action action" data-v-d9c85796><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-d9c85796><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="vt-menu-group" data-v-d9c85796><div class="vt-menu-item item" data-v-d9c85796><div class="vt-social-links social-links" data-v-d9c85796><!--[--><a class="vt-social-link is-small" href="https://github.com/hyaliyun/MIT/" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><button type="button" class="vt-hamburger VPNavBarHamburger hamburger" aria-label="Mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-78ea45ed data-v-dcc88df6><span class="vt-hamburger-container"><span class="vt-hamburger-top"></span><span class="vt-hamburger-middle"></span><span class="vt-hamburger-bottom"></span></span></button></div></div></div><!----></header><!----><!----><div id="VPContent" class="VPContent" data-v-e4982c5a data-v-8b82bdb3><div class="VPContentPage" data-v-8b82bdb3><main><div style="position:relative;"><div><section data-v-91039cb8><div class="top-banner" data-v-91039cb8><div class="top-banner-title" data-v-91039cb8><div class="top-banner-title-text" data-v-91039cb8>prompts chat</div></div></div><div class="search-container" data-v-91039cb8><span class="search-icon" data-v-91039cb8></span><input type="text" class="search-input" value="" placeholder="Search..." data-v-91039cb8><!----></div><div class="card-container" data-v-91039cb8><!--[--><div class="poem-container" data-v-91039cb8 data-v-47d7db5a><div class="review" data-v-47d7db5a><div class="review-title" data-v-47d7db5a><span class="icon" data-v-47d7db5a>question:</span>GPT-4 Turbo: Research Preview (128K token limit, Short-Term Availability) If this app doesn&#39;t respond, it&#39;s likely due to our API key hitting the daily limit of our organization. Consider trying our GPT-3.5 app: https://huggingface.co/spaces/yuntian-deng/ChatGPT Chatbot Explain this code like i am a layperson: # Extracting LoRA adapters from model diff using SVD Low-rank adaptation (LoRA) is a popular technique for parameter efficient training (PEFT) of neural networks. It is usually used together with gradient descent in order to fine-tune models to perform specific task. However it can be also derived from already trained weights using a technique called Singular value decomposition. Inspired by an already existing technique for Stable Diffusion, we implement LoRA extraction for transformers language models. First, lets choose a base model and a target model we want to extract a LoRA adapter from. I think its probably better to use the model that was used as the base model before our target model was fine-tuned (so in this instance, as our target model valine/OpenPirate was trained on top of teknium/OpenHermes-2.5-Mistral-7B I choose OpenHermes instead of the foundation Mistral 7B model as base model) base_model_id = teknium/OpenHermes-2.5-Mistral-7B target_model_id = valine/OpenSnark setup enviroment !pip install torch transformers huggingface_hub bitsandbytes accelerate peft sentencepiece protobuf pyyaml safetensors tqdm Model download Define a utility function to download transformer models from the Hugging Face Hub, prefering .safetensors files over PyTorch .bin files. import os from huggingface_hub import list_repo_files, snapshot_download def download_transformers_model(repo_id, cache_dir=None): # Check for .safetensors files in the repository repo_files = list_repo_files(repo_id) has_safetensors = any(file.endswith(.safetensors) for file in repo_files) # Define ignore_patterns based on the presence of .safetensors files ignore_patterns = [.bin&quot;] if has_safetensors else None # Download the repository, ignoring PyTorch .bin files if .safetensors files are present local_path = snapshot_download(repo_id=repo_id, cache_dir=cache_dir, ignore_patterns=ignore_patterns, ) print(f&quot;Model downloaded to: {local_path}) if has_safetensors: print(Note: PyTorch .bin files were ignored due to the presence of .safetensors files.) return os.path.abspath(local_path), has_safetensors # Downloading the base model cache_dir = ./models base_model_download_path, base_model_has_safetensors = download_transformers_model(base_model_id, cache_dir) models = { base : { download_path : base_model_download_path, has_safetensors : base_model_has_safetensors }, target : None } # Identifying relevant model layers Define functions to identify linear and embedding layers within transformer models. These layers are targets for LoRA adapters extraction. # This code has been modified from its original version on the Axolotl project. # Copyright 2023 Axolotl contributors. # Licensed under the Apache License, Version 2.0 (the License); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an AS IS BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import os import torch import torch import bitsandbytes as bnb from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer from peft.tuners.lora import QuantLinear def get_linear_embedding_layers(model_type):  returns the linear embedding layers needed for loras, dependent on the model arch  if model_type == gpt_neox: return [embed_in, embed_out] if model_type == falcon: return [word_embeddings, lm_head] return [embed_tokens, lm_head] def find_all_linear_names(model): cls = (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear, QuantLinear) names = [] for name, module in model.named_modules(): if ( isinstance(module, cls) or Linear in module.class.name and module.class.name not in (LlamaLinearScalingRotaryEmbedding,) ): names.append(name) return names def get_linear_module_names(model_id): model = AutoModelForCausalLM.from_pretrained(model_id, state_dict={}, device_map=meta) #avoid loading weights as we wont need them return find_all_linear_names(model) linear_module_names = get_linear_module_names(models[base][download_path]) # Downloading the target model target_model_download_path, target_model_has_safetensors = download_transformers_model(target_model_id, cache_dir) models[target] = { download_path : target_model_download_path, has_safetensors : target_model_has_safetensors } # Loading tensors from .bin files Define functions to load PyTorch tensors from .bin files or .safetensors file. import torch import glob def load_pytorch_tensors(directory, device=cpu):  Loads tensors from .bin files in the specified directory into a dictionary. Args: - directory (str): Path to the directory containing .bin files. - device (str): The device to load the tensors on (cpu, cuda, etc.). Default is cpu. Returns: - dict: A dictionary containing all tensors from the .bin files.  tensors_dict = {} # Use glob to find all .bin files in the directory file_paths = glob.glob(f{directory}/.bin) # Loop through each file and load its tensors into the dictionary for file_path in sorted(file_paths): loaded_tensors = torch.load(file_path, map_location=torch.device(device)) for k, v in loaded_tensors.items(): tensors_dict[k] = v return tensors_dict import glob from safetensors import safe_open def load_safetensors(directory, framework=pt, device=cpu):  Loads tensors from .safetensors files in the specified directory into a dictionary. Args: - directory (str): Path to the directory containing .safetensors files. - framework (str): The framework to use (pt for PyTorch, tf for TensorFlow, etc.). Default is pt. - device (str): The device to load the tensors on (cpu, cuda, etc.). Default is cpu. Returns: - dict: A dictionary containing all tensors from the .safetensors files.  tensors_dict = {} # Use glob to find all .safetensors files in the directory file_paths = glob.glob(f&quot;{directory}/.safetensors&quot;) # Loop through each file and load its tensors into the dictionary for file_path in sorted(file_paths): with safe_open(file_path, framework=framework, device=device) as f: for k in f.keys(): tensors_dict[k] = f.get_tensor(k) return tensors_dict # Loading model weights Load weights for both base and target models base_model_weights = load_safetensors(models[base][download_path]) if models[base][has_safetensors] else load_pytorch_tensors(models[base][download_path]) target_model_weights = load_safetensors(models[target][download_path]) if models[target][has_safetensors] else load_pytorch_tensors(models[target][download_path]) # Weight matrix decomposition The crux of what were doing here. We define a function to decompose weight matrices into low-rank matrices using SVD import torch def _low_rank_decomposition(weight, reduced_rank=16):  Decompose a 2D matrix into low-rank matrices A and B using SVD.a :param weight: The matrix to decompose, of shape (H, W) :param reduced_rank: The final rank of the decomposition :return: A tuple of tensors (A, B)  if weight.dim() != 2: raise ValueError(f&quot;Only support 2D matrix, but your input has {weight.dim()} dimensions.) # SVD Decomposition U, S, Vh = torch.linalg.svd(weight, full_matrices=False) # Truncated matrices A = Vh[:reduced_rank, :] B = U[:, :reduced_rank] @ torch.diag(S[:reduced_rank]) return A, B def decompose_delta_weight(new_weight, base_weight, alpha, reduced_rank, device=None): if device is None: device = cuda if torch.cuda.is_available() else cpu new_weight = new_weight.to(device) base_weight = base_weight.to(device)  Decompose the delta weight into low-rank matrices A and B, considering the alpha scaling factor. :param new_weight: The updated weight matrix after applying LoRA. :param base_weight: The original weight matrix before LoRA. :param alpha: The alpha scaling factor used in LoRA. :param reduced_rank: The rank for the low-rank decomposition. :return: A tuple of tensors (A, B)  delta_weight = new_weight - base_weight # Check if alpha is applied uniformly # Adjust the implementation if alpha is applied differently adjusted_delta_weight = delta_weight / alpha A, B = low_rank_decomposition(adjusted_delta_weight, reduced_rank=reduced_rank) return A, B Extract the LoRAs from tqdm.notebook import tqdm loras = { } # lower rank captures less of the original model, a rank of 32 is probably reasonable for small delta (task specific finetunes and such) alpha = 1 rank = 32 for module in tqdm(linear_module_names): target_tensor = target_model_weights[module+.weight] base_tensor = base_model_weights[module+.weight] lora_A, lora_B = decompose_delta_weight(target_tensor, base_tensor, alpha, rank) loras[f&quot;base_model.model.{module}.lora_A.weight&quot;] = lora_A.to(cpu) loras[f&quot;base_model.model.{module}.lora_B.weight&quot;] = lora_B.to(cpu) # Extracting correct module names for PEFT PEFT config uses partial module names, lets extract them correctly def get_module_peft_name(module_name): return module_name.split(.)[-1] # Configuring LoRA model with PEFT Set up a PEFT LoRA configuration for the model. Load the base model and apply this configuration, saving the configuration on disk. The LoRA weights will be saved later from our SVD decomposition. from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer from peft import get_peft_model, LoraConfig LORA_OUT_DIR = ./lora lora_config = LoraConfig( lora_alpha=32, # Setting the alpha to the to decomposition rank value (instead of alpha value used) seems to give better performance. Further testing would be needed to understand what is the optimal alpha value to use lora_dropout=0, r=32, bias=none, task_type=CAUSAL_LM, target_modules= list(set([get_module_peft_name(e) for e in linear_module_names])), ) model = AutoModelForCausalLM.from_pretrained(models[base][download_path], load_in_4bit=True) peft_model = get_peft_model(model, lora_config) # Save to disk peft_model.save_pretrained(LORA_OUT_DIR) del peft_model # Saving LoRA adapters as SafeTensors Save the decomposed LoRA weights along our PEFT adapter config import torch from safetensors.torch import save_file for key in loras.keys(): loras[key] = loras[key].to(cpu).contiguous() save_file(loras, os.path.join(LORA_OUT_DIR, adapter_model.safetensors)) # Testing the result Load the LoRA adapters from its saved location. If everything went well, this model base model + extracted adapter will behave like the target module. Lets try and generate responses with it. import os from peft import PeftModel, PeftConfig from transformers import AutoModelForCausalLM config = PeftConfig.from_pretrained(os.path.abspath(LORA_OUT_DIR)) model = AutoModelForCausalLM.from_pretrained(models[base][download_path], load_in_4bit=True) model = PeftModel.from_pretrained(model, os.path.abspath(LORA_OUT_DIR)) tokenizer = AutoTokenizer.from_pretrained(models[base][download_path], use_fast=True) # Test input messages = [ { role: system, content: You are a human just going about your day., }, {role: user, content: Hey whats up?}, ] # Tokenize and format the chat for the model tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=pt) # Generate a response outputs = model.generate(tokenized_chat, max_new_tokens=128) # Adjust max_new_tokens if needed response_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(response_text) Seems snarky to me! It works! Uploading the extracted adapter to Hugging Face Hub First, lets replace the base_model_name_or_path value of the adapter config with the base model id instead of the local path import os import json adapter_config_path = os.path.join(LORA_OUT_DIR, adapter_config.json) # Load the configuration from the file with open(adapter_config_path, r) as file: config = json.load(file) # Update the base_model_name_or_path in the configuration config[base_model_name_or_path] = base_model_id # Save the updated configuration back to the file with open(adapter_config_path, w) as file: json.dump(config, file, indent=2) print(Configuration updated successfully.) Now lets create a readme import yaml # Define your metadata as a Python dictionary metadata = { library_name: peft, base_model: base_model_id } # Convert the dictionary to YAML format yaml_frontmatter = yaml.dump(metadata, sort_keys=False) # Define your Markdown content markdown_content = f&quot; # Low-rank decomposition of {target_model_id} using {base_model_id} as base Created using LoRD  # Combine the YAML frontmatter and Markdown content full_content = fn{yaml_frontmatter}n{markdown_content} adapter_readme_path = os.path.join(LORA_OUT_DIR, README.md) # Write to a Markdown file with open(adapter_readme_path, w) as md_file: md_file.write(full_content) print(Markdown file successfully created.) from huggingface_hub import notebook_login notebook_login() from huggingface_hub import HfApi # Initialize the API hf_api = HfApi() # Get the current users username user_info = hf_api.whoami() username = user_info[name] # Define your model name and the local directory path model_name = input(Enter your desired model name: ) repo_id = f{username}/{model_name} # Create the repository on the Hugging Face Hub, setting it to private try: # This will return the URL of the newly created repo if successful repo_url = hf_api.create_repo(repo_id=repo_id, exist_ok=True) print(f&quot;Repository {repo_id} created or already exists on the Hugging Face Hub.) except Exception as e: print(f&quot;Error creating repository: {e}) exit(1) # Exit if theres an issue creating the repo # Upload all the content from the local folder to your remote repository try: hf_api.upload_folder(folder_path=LORA_OUT_DIR, repo_id=repo_id, repo_type=model) print(f&quot;All files in {LORA_OUT_DIR} have been uploaded to {repo_url.url} successfully.) except Exception as e: prLoading Skip to main content LoRD.ipynb LoRD.ipynb Extracting LoRA adapters from model diff using SVD Low-rank adaptation (LoRA) is a popular technique for parameter efficient training (PEFT) of neural networks. It is usually used together with gradient descent in order to fine-tune models to perform specific task. However it can be also derived from already trained weights using a technique called Singular value decomposition. Inspired by an already existing technique for Stable Diffusion, we implement LoRA extraction for transformers language models. First, lets choose a base model and a target model we want to extract a LoRA adapter from. I think its probably better to use the model that was used as the base model before our target model was fine-tuned (so in this instance, as our target model valine/OpenPirate was trained on top of teknium/OpenHermes-2.5-Mistral-7B I choose OpenHermes instead of the foundation Mistral 7B model as base model) [ ] base_model_id = teknium/OpenHermes-2.5-Mistral-7B target_model_id = valine/OpenSnark setup enviroment [ ] !pip install torch transformers huggingface_hub bitsandbytes accelerate peft sentencepiece protobuf pyyaml safetensors tqdm Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118) Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2) Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3) Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0) Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2) Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.8.2) Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99) Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.2) Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1) Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.2) Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2) Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0) Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12) Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2) Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0) Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25) Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0) Requirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2) Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.12.0) Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (2.1.2) Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.1.1) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4) Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (1.26.13) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2022.12.7) Requirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch) (1.3.0) WARNING: Running pip as the root user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv [notice] A new release of pip is available: 23.3.1 -&gt; 24.0 [notice] To update, run: python -m pip install --upgrade pip Model download Define a utility function to download transformer models from the Hugging Face Hub, prefering .safetensors files over PyTorch .bin files. [ ] import os from huggingface_hub import list_repo_files, snapshot_download def download_transformers_model(repo_id, cache_dir=None): # Check for .safetensors files in the repository repo_files = list_repo_files(repo_id) has_safetensors = any(file.endswith(.safetensors) for file in repo_files) # Define ignore_patterns based on the presence of .safetensors files ignore_patterns = [.bin&quot;] if has_safetensors else None # Download the repository, ignoring PyTorch .bin files if .safetensors files are present local_path = snapshot_download(repo_id=repo_id, cache_dir=cache_dir, ignore_patterns=ignore_patterns, ) print(f&quot;Model downloaded to: {local_path}) if has_safetensors: print(Note: PyTorch .bin files were ignored due to the presence of .safetensors files.) return os.path.abspath(local_path), has_safetensors Downloading the base model [ ] cache_dir = ./models base_model_download_path, base_model_has_safetensors = download_transformers_model(base_model_id, cache_dir) models = { base : { download_path : base_model_download_path, has_safetensors : base_model_has_safetensors }, target : None } Identifying relevant model layers Define functions to identify linear and embedding layers within transformer models. These layers are targets for LoRA adapters extraction. [ ] # This code has been modified from its original version on the Axolotl project. # Copyright 2023 Axolotl contributors. # Licensed under the Apache License, Version 2.0 (the License); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # http://www.apache.org/licenses/LICENSE-2.0 # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an AS IS BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import os import torch import torch import bitsandbytes as bnb from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer from peft.tuners.lora import QuantLinear def get_linear_embedding_layers(model_type):  returns the linear embedding layers needed for loras, dependent on the model arch  if model_type == gpt_neox: return [embed_in, embed_out] if model_type == falcon: return [word_embeddings, lm_head] return [embed_tokens, lm_head] def find_all_linear_names(model): cls = (bnb.nn.Linear4bit, bnb.nn.Linear8bitLt, torch.nn.Linear, QuantLinear) names = [] for name, module in model.named_modules(): if ( isinstance(module, cls) or Linear in module.class.name and module.class.name not in (LlamaLinearScalingRotaryEmbedding,) ): names.append(name) return names def get_linear_module_names(model_id): model = AutoModelForCausalLM.from_pretrained(model_id, state_dict={}, device_map=meta) #avoid loading weights as we wont need them return find_all_linear_names(model) linear_module_names = get_linear_module_names(models[base][download_path]) Downloading the target model [ ] target_model_download_path, target_model_has_safetensors = download_transformers_model(target_model_id, cache_dir) models[target] = { download_path : target_model_download_path, has_safetensors : target_model_has_safetensors } Loading tensors from .bin files Define functions to load PyTorch tensors from .bin files or .safetensors file. [ ] import torch import glob def load_pytorch_tensors(directory, device=cpu):  Loads tensors from .bin files in the specified directory into a dictionary. Args: - directory (str): Path to the directory containing .bin files. - device (str): The device to load the tensors on (cpu, cuda, etc.). Default is cpu. Returns: - dict: A dictionary containing all tensors from the .bin files.  tensors_dict = {} # Use glob to find all .bin files in the directory file_paths = glob.glob(f{directory}/.bin&quot;) # Loop through each file and load its tensors into the dictionary for file_path in sorted(file_paths): loaded_tensors = torch.load(file_path, map_location=torch.device(device)) for k, v in loaded_tensors.items(): tensors_dict[k] = v return tensors_dict [ ] import glob from safetensors import safe_open def load_safetensors(directory, framework=pt, device=cpu):  Loads tensors from .safetensors files in the specified directory into a dictionary. Args: - directory (str): Path to the directory containing .safetensors files. - framework (str): The framework to use (pt for PyTorch, tf for TensorFlow, etc.). Default is pt. - device (str): The device to load the tensors on (cpu, cuda, etc.). Default is cpu. Returns: - dict: A dictionary containing all tensors from the .safetensors files.  tensors_dict = {} # Use glob to find all .safetensors files in the directory file_paths = glob.glob(f&quot;{directory}/.safetensors&quot;) # Loop through each file and load its tensors into the dictionary for file_path in sorted(file_paths): with safe_open(file_path, framework=framework, device=device) as f: for k in f.keys(): tensors_dict[k] = f.get_tensor(k) return tensors_dict Loading model weights Load weights for both base and target models [ ] base_model_weights = load_safetensors(models[base][download_path]) if models[base][has_safetensors] else load_pytorch_tensors(models[base][download_path]) target_model_weights = load_safetensors(models[target][download_path]) if models[target][has_safetensors] else load_pytorch_tensors(models[target][download_path]) Weight matrix decomposition The crux of what were doing here. We define a function to decompose weight matrices into low-rank matrices using SVD [ ] import torch def _low_rank_decomposition(weight, reduced_rank=16):  Decompose a 2D matrix into low-rank matrices A and B using SVD.a :param weight: The matrix to decompose, of shape (H, W) :param reduced_rank: The final rank of the decomposition :return: A tuple of tensors (A, B)  if weight.dim() != 2: raise ValueError(f&quot;Only support 2D matrix, but your input has {weight.dim()} dimensions.) # SVD Decomposition U, S, Vh = torch.linalg.svd(weight, full_matrices=False) # Truncated matrices A = Vh[:reduced_rank, :] B = U[:, :reduced_rank] @ torch.diag(S[:reduced_rank]) return A, B def decompose_delta_weight(new_weight, base_weight, alpha, reduced_rank, device=None): if device is None: device = cuda if torch.cuda.is_available() else cpu new_weight = new_weight.to(device) base_weight = base_weight.to(device)  Decompose the delta weight into low-rank matrices A and B, considering the alpha scaling factor. :param new_weight: The updated weight matrix after applying LoRA. :param base_weight: The original weight matrix before LoRA. :param alpha: The alpha scaling factor used in LoRA. :param reduced_rank: The rank for the low-rank decomposition. :return: A tuple of tensors (A, B)  delta_weight = new_weight - base_weight # Check if alpha is applied uniformly # Adjust the implementation if alpha is applied differently adjusted_delta_weight = delta_weight / alpha A, B = _low_rank_decomposition(adjusted_delta_weight, reduced_rank=reduced_rank) return A, B Extract the LoRAs [ ] from tqdm.notebook import tqdm loras = { } # lower rank captures less of the original model, a rank of 32 is probably reasonable for small delta (task specific finetunes and such) alpha = 1 rank = 32 for module in tqdm(linear_module_names): target_tensor = target_model_weights[module+.weight&quot;] base_tensor = base_model_weights[module+.weight] lora_A, lora_B = decompose_delta_weight(target_tensor, base_tensor, alpha, rank) loras[f&quot;base_model.model.{module}.lora_A.weight&quot;] = lora_A.to(cpu) loras[f&quot;base_model.model.{module}.lora_B.weight&quot;] = lora_B.to(cpu) Extracting correct module names for PEFT PEFT config uses partial module names, lets extract them correctly [ ] def get_module_peft_name(module_name): return module_name.split(.)[-1] Configuring LoRA model with PEFT Set up a PEFT LoRA configuration for the model. Load the base model and apply this configuration, saving the configuration on disk. The LoRA weights will be saved later from our SVD decomposition. [ ] from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer from peft import get_peft_model, LoraConfig LORA_OUT_DIR = ./lora lora_config = LoraConfig( lora_alpha=32, # Setting the alpha to the to decomposition rank value (instead of alpha value used) seems to give better performance. Further testing would be needed to understand what is the optimal alpha value to use lora_dropout=0, r=32, bias=none, task_type=CAUSAL_LM, target_modules= list(set([get_module_peft_name(e) for e in linear_module_names])), ) model = AutoModelForCausalLM.from_pretrained(models[base][download_path], load_in_4bit=True) peft_model = get_peft_model(model, lora_config) # Save to disk peft_model.save_pretrained(LORA_OUT_DIR) del peft_model Saving LoRA adapters as SafeTensors Save the decomposed LoRA weights along our PEFT adapter config [ ] import torch from safetensors.torch import save_file for key in loras.keys(): loras[key] = loras[key].to(cpu).contiguous() save_file(loras, os.path.join(LORA_OUT_DIR, adapter_model.safetensors)) Testing the result Load the LoRA adapters from its saved location. If everything went well, this model base model + extracted adapter will behave like the target module. Lets try and generate responses with it. [ ] import os from peft import PeftModel, PeftConfig from transformers import AutoModelForCausalLM config = PeftConfig.from_pretrained(os.path.abspath(LORA_OUT_DIR)) model = AutoModelForCausalLM.from_pretrained(models[base][download_path], load_in_4bit=True) model = PeftModel.from_pretrained(model, os.path.abspath(LORA_OUT_DIR)) tokenizer = AutoTokenizer.from_pretrained(models[base][download_path], use_fast=True) # Test input messages = [ { role: system, content: You are a human just going about your day., }, {role: user, content: Hey whats up?}, ] # Tokenize and format the chat for the model tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=pt) # Generate a response outputs = model.generate(tokenized_chat, max_new_tokens=128) # Adjust max_new_tokens if needed response_text = tokenizer.decode(outputs[0], skip_special_tokens=True) print(response_text) Seems snarky to me! It works! Uploading the extracted adapter to Hugging Face Hub First, lets replace the base_model_name_or_path value of the adapter config with the base model id instead of the local path [ ] import os import json adapter_config_path = os.path.join(LORA_OUT_DIR, adapter_config.json) # Load the configuration from the file with open(adapter_config_path, r) as file: config = json.load(file) # Update the base_model_name_or_path in the configuration config[base_model_name_or_path] = base_model_id # Save the updated configuration back to the file with open(adapter_config_path, w) as file: json.dump(config, file, indent=2) print(Configuration updated successfully.) Configuration updated successfully. Now lets create a readme [ ] import yaml # Define your metadata as a Python dictionary metadata = { library_name: peft, base_model: base_model_id } # Convert the dictionary to YAML format yaml_frontmatter = yaml.dump(metadata, sort_keys=False) # Define your Markdown content markdown_content = f&quot; # Low-rank decomposition of {target_model_id} using {base_model_id} as base Created using LoRD  # Combine the YAML frontmatter and Markdown content full_content = fn{yaml_frontmatter}n{markdown_content} adapter_readme_path = os.path.join(LORA_OUT_DIR, README.md) # Write to a Markdown file with open(adapter_readme_path, w) as md_file: md_file.write(full_content) print(Markdown file successfully created.) Markdown file successfully created. [ ] from huggingface_hub import notebook_login notebook_login() [ ] from huggingface_hub import HfApi # Initialize the API hf_api = HfApi() # Get the current users username user_info = hf_api.whoami() username = user_info[name] # Define your model name and the local directory path Colab paid products - Cancel contracts here int(f&quot;Error uploading files to the repository: {e}) Let me simplify that for you! Imagine you have a very complicated recipe book (a machine learning model) that helps you cook exactly what you want (generate text, recognize images, etc.). However, this book is huge, and you only want to cook something very specific. Copying the entire book would be too much work and not necessary. Instead, you find a clever way to copy only the necessary parts to tweak your existing recipes slightly and achieve the new dish youre aiming for. This method of copying only whats needed and integrating it with your current recipes is known as Low-rank adaptation (LoRA). Heres how the code you shared works, using this analogy: 1. Choosing the Recipe Books: It starts by selecting two recipe books - a base book (teknium/OpenHermes-2.5-Mistral-7B), which you already know, and a target book (valine/OpenSnark), which has the new recipe you want. 2. Preparation: Before you start, you ensure you have all the tools and ingredients ready ( pip install  is like making sure you have all the kitchen tools and ingredients). 3. Downloading the Books: The code then downloads these recipe books from a huge library (Hugging Face Hub). 4. Identifying Special Ingredients: It looks through the books to find the special ingredients and methods (linear and embedding layers) that make the new recipe unique. 5. Getting the New Ingredients: The code downloads the target book with the new recipe. 6. Reading the Ingredients: It reads through the books to understand the ingredients and methods used (loading tensors). 7. Creating a New Recipe: The most magical part comes next - the code combines ingredients from both books to create a new, simplified recipe. This process is called low-rank decomposition, which is a fancy way of saying it finds a simpler way to combine the complex methods. 8. Extracting the Essence: The new recipe is then saved (Extract the LoRAs) in a simple format so you can use it without the entire book. 9. Testing the Dish: To make sure the new recipe works, the code tries it out by cooking something (Testing the result). 10. Sharing Your Recipe: Happy with the new dish? The code then helps you share this recipe with others by uploading it to the library (Hugging Face Hub) so others can try it too. So, essentially, this code cleverly extracts a specific taste or style from one big recipe book (model) and makes it easy to apply to your dishes without needing to copy the entire book! CONTINUE CONVERSATION: What if I already have a Low-rank decomposition of a model (specifically, a model called &#39;quietstar-8-ahead&#39; which uses &#39;mistralai/Mistral-7B-v0.1&#39; as a base) It was Created using LoRD. For context, the &#39;quietstar-8-ahead&#39; model is a result of this paper: &quot;&quot;Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking Eric Zelikman Stanford University Georges Harik Notbad AI Inc Yijia Shao Stanford University Varuna Jayasiri Notbad AI Inc Nick Haber Stanford University Noah D. Goodman Stanford University Abstract When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting  ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thoughts start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LMs ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9%10.9%) and CommonsenseQA (36.3%47.2%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way. Life can only be understood backwards; but it must be lived forwards.  Sren Kierkegaard 1 Introduction Much of the meaning of text is hidden between the lines: without understanding why statements appear in a document, a reader has only a shallow understanding. Moreover, this has been repeatedly shown to be true for LMs as well, in the contexts of tasks ranging from commonsense reasoning to theorem proving to programming (Wei et al., 2022b; Nye et al., 2021; Zelikman et al., 2022; 2023a; Kojima et al., 2022). Reasoning about implications of text to predict later text has consistently been shown to improve LM performance on a variety of tasks, but methods for allowing LMs to learn from their reasoning (e.g., Zelikman et al. 2022) have focused on solving individual tasks or predefined sets of tasks (e.g., Wei et al. 2021b). These works rely on carefully curated datasets to provide either specific reasoning tasks or in some cases, the reasoning itself. We instead ask, if reasoning is implicit in all text, why shouldnt we leverage the task of language modeling to teach reasoning? 1 arXiv:2403.09629v2 [cs.CL] 18 Mar 2024 START + Sampled Thought 4 + 1 END LM LM LM 1 Discard 2 + 1 2 START + 1) Think = 5 4 n 2 2 END Thought1 Thought3 Thought4 Thought5 Thought7 + LM LM LM LM LM Reward Harms Prediction Helps Prediction LM Thought2 LM LM LM LM LM 4 Thought0 LM 2) Talk 3) Learn Original Text Sampled Thought Thought6 Figure 1: Quiet-STaR. We visualize the algorithm as applied during training to a single thought. We generate thoughts, in parallel, following all tokens in the text (think). The model produces a mixture of its next-token predictions with and without a thought (talk). We apply REINFORCE, as in STaR, to increase the likelihood of thoughts that help the model predict future text while discarding thoughts that make the future text less likely (learn). In particular, the Self-Taught Reasoner (STaR, Zelikman et al. 2022) showed that LMs can bootstrap their reasoning ability on question-answering (QA) datasets by sampling rationales to attempt to answer questions, training on rationales if they led to a correct final answer, and then repeating this to iteratively solve more difficult problems. Yet, training from curated QA datasets limits the scale and generalizability of the rationales. QA datasets, especially high-quality ones, require thoughtful curation and will inherently only ever cover a subset of reasoning tasks. Thus, we extend STaR  instead of the LM learning to reason on particular tasks like mathematical QA, we train an LM to generate reasoning that helps it infer future text from a large internet text corpus. As a result, we allow the LM to learn from the diverse tasks present in language (Weber et al., 2021). This builds on an intuition essential to the current language modeling paradigm, namely, that language models are unsupervised multitask learners (Radford et al., 2019). Thus, as in STaR, we leverage the LMs pre-existing reasoning ability to generate rationales and train the LM on them with a REINFORCE-based reward (Williams, 1992). We refer to this technique as Quiet-STaR, as it can be understood as applying STaR quietly, training the model to think before it speaks. Broadly, Quiet-STaR proceeds by generating rationales after every token to explain future text (think), mixing the future-text predictions with and without rationales (talk), and then learning to generate better rationales using REINFORCE (learn). We apply Quiet-STaR to Mistral 7B (Jiang et al., 2023) using the web text datasets OpenWebMath (Paster et al., 2023) and Colossal Clean Crawled Corpus (C4, Raffel et al. 2020). We find that, even without dataset-specific fine-tuning, Quiet-STaR results in improvements to zero-shot directreasoning abilities on CommonsenseQA (36.3%47.2%) and GSM8K (5.9%10.9%), and that these improvements consistently increase with the number of tokens used in the LMs internal thoughts. Lastly, we qualitatively investigate patterns in the generated rationales. In solving this task, we make the following contributions: 1. We generalize STaR to learn reasoning from diverse unstructured text data. To our knowledge, this is the first work explicitly training LMs to reason generally from text, rather than on curated reasoning tasks or collections of reasoning tasks. 2. We propose and implement a parallel sampling algorithm that makes our training procedure scalable, generating rationales from all token positions in a given string. 3. We introduce custom meta-tokens at the start and end of each thought to allow the LM to learn that it should be generating a rationale and when it should make a prediction based on that rationale. 4. We apply a mixing head to retrospectively determine how much to incorporate the next-token prediction from a given thought into the current next-token prediction. 5. We show that a non-myopic loss, including multiple tokens ahead for language modeling, improves the effect of thinking. 6. On multiple tasks, we demonstrate that thinking allows the LM to predict difficult tokens better than one trained on the same web text, improving with longer thoughts. 2 10 20 30 40 50 60 70 80 90 100 Training Step 6% 7% 8% 9% 10% 11% GSM8K Accuracy (%) # Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (a) GSM8K 10 20 30 40 50 60 70 80 90 100 Training Step 34% 36% 38% 40% 42% 44% 46% CommonsenseQA Accuracy (%) # Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (b) CommonsenseQA Figure 2: Generalization Results. We evaluate the extent to which the model trained with Quiet-STaR generalizes to directly answering problems that require reasoning. The left plot (a) shows the zero-shot accuracy on GSM8K, while the right plot (b) shows the zero-shot accuracy on CommonsenseQA, without any fine-tuning. In both plots, the x-axis represents training steps, and each line corresponds to a different number of thinking tokens used during Quiet-STaR training. The y-axis measures the zero-shot direct accuracy on the respective datasets. We also include an inference normalized version of this plot in Figure 6. 2 Related Work 2.1 Reasoning in Language Models There have been many works on training and exploiting language models to solve difficult tasks by first training them to reason through them. For example, Rajani et al. (2019) demonstrated that a pre-trained language model fine-tuned to output on human reasoning traces before answering multiple-choice commonsense reasoning questions outperformed one trained directly on answers. Shwartz et al. (2020) demonstrated that language models, when provided with some scaffolding, can generate these helpful chain-of-thought solutions without additional supervision. Later, Nye et al. (2021) demonstrated that scratchpads required less scaffolding when the language models were more capable, a result later reinforced by Wei et al. (2022b), emphasizing informal tasks, and further strengthened by Kojima et al. (2022), demonstrating this behavior could be accomplished zero-shot. Most recently, Wang &amp; Zhou (2024) showed further that for commonsense-question answering, one could force a language model to leverage chain-of-thought reasoning by preventing it from emitting any valid answer tokens unless it was confident. However, once again, these approaches only work for a question-answer dataset, and Wang &amp; Zhou (2024) relies on heuristics to identify when the model has output answer tokens. Somewhat like TRICE (Phan et al., 2023), we use the relative improvements in the log-likelihood of the target text across rationales as an estimate of quality, but we simply subtract the mean reward and do not incorporate more complex control variates. 2.2 Training Language Models to Reason One direction that researchers have used to train language models to reason or improve their reasoning is training the language model on mined reasoning traces or reasoning-like data (Rajani et al., 2019; Wei et al., 2021a; Lewkowycz et al., 2022; Chung et al., 2022; Gunasekar et al., 2023). Although this approach has been demonstrated to be effective, it comes with drawbacks. It requires either manual annotation, which is sensitive to the capability of the annotators and is off-policy for the language model (i.e., the distribution of reasoning is not text that the language model would otherwise likely have generated). This approach is also expensive, difficult to scale, and provides no clear path to solving problems harder than those that the annotators are capable of solving. Another direction for teaching reasoning relies on a language models own generated reasoning, which can be seen as building on a large body of literature on self-play (Silver et al., 2017; Anthony et al., 2017; Polu &amp; Sutskever, 2020). These include methods such as the 3 Algorithm 1: Quiet Self-Taught Reasoner (Quiet-STaR) Input: Language model 0, training steps num steps, sequence length l, thought length t, learning rate , batch size b, number of thoughts nthoughts, number of ground truth tokens used for supervising each thought ntrue Output: Language model  that generates rationales to predict future text for i = 0 to num steps do Sample batch of sequences X of length l h init  hidden statesi (X) for j = 1 to l in parallel using attention mask do log p init j:j+ntrue  lm headi (h init j:j+ntrue ) // Predict next tokens Tj  generate tokensi ([X:j ; &lt;start thought&gt;], t, nthoughts) // Generate thought Tj  [Tj ; &lt;end thought&gt;] h thought j:j+ntrue  hidden statesi ([X:j ; Tj ; Xj:j+ntrue1 ]) log p thought j:j+ntrue  lm headi (h thought j:j+ntrue ) // Predict next tokens w/ thought wj:j+ntrue  mixing headi (h thought j:j+ntrue , h init j:j+ntrue ) log p talk j  wj:j+ntrue  log p init j:j+ntrue + (1  wj:j+ntrue )  log p thought j:j+ntrue // Mix logits L NLL j   log p talk j:j+ntrue (Xj+1:j+ntrue+1) rj = log p talk j:j+ntrue (Xj+1:j+ntrue+1)  log p talk j:j+ntrue (Xj+1:j+ntrue+1) L REINFORCE j  rj1[rj &gt; 0]   log pi (Tj |[X:j ; &lt;start thought&gt;]) Lj  L NLL j + L REINFORCE j i+1  i    l j=1 Lj // Update model parameters return num steps Self-Taught Reasoner (Zelikman et al., 2022), which demonstrated that a language model iteratively trained on its reasoning that led to correct answers could solve increasingly difficult problems. Later work aimed to leverage additional information or assumptions such as Huang et al. (2022) which demonstrated that the algorithm proposed in STaR could still work if one assumed that the majority-vote answer was correct (although this has a lower ultimate performance). Further work has generalized the results of Zelikman et al. (2022), such as Uesato et al. (2022) which demonstrated additional usefulness to process-based supervision where incorrect reasoning traces were filtered, recently VSTaR (Hosseini et al., 2024) that demonstrates that training a verifier to guide generation also improves performance, as well as TRICE (Hoffman et al., 2024) which maximizes the marginal likelihood of the correct answer given several reasoning traces per problem. Finally, related work has also explored learning intermediate reasoning in the constrained setting of making mathematical statements, where statements in the models intermediate reasoning could be constrained to only be valid mathematical statements (Poesia et al., 2023). We include further discussion of related reasoning works in Appendix F. 2.3 Meta-tokens Recently, a growing body of work has demonstrated the usefulness of custom tokens optimized to perform specific functions in the context of a neural network  for this reason, they have also been referred to as function vectors. (Todd et al., 2023). One of the original instantiations of this was prompt-tuning (Lester et al., 2021) (and relatedly prefix-tuning (Li &amp; Liang, 2021)), where the embeddings corresponding to the tokens of a prompt could be optimized to better accomplish a task. Others have applied meta-tokens to compress long prompts (Li et al., 2023; Jung &amp; Kim, 2023) for efficiency. Most relevant to this work, Mu et al. (2024) optimized a token such that, when the tokens after it could not attend to the tokens before it (i.e., a context compression token), it would provide sufficient information to future tokens. Although we do not focus on compression, we share the problem of learning a 4 token that affects attention and controls complex downstream behavior. In one related work, Goyal et al. (2023) show that learning a single pause token (essentially representing each token as two tokens) improves LM performance. However, unlike the thought tokens in our work, this pause token does not initialize a thought  instead, it can be seen as acting as the entirety of the thought. We find that reasoning in language is significantly more helpful. 3 Problem Statement In this work, we introduce an auxiliary rationale variable between each pair of observed tokens of the sequence. We then aim to optimize a language model with parameters  with the capacity to generate intermediate thoughts (or rationales) such that   = arg max Ex [logp (xi:n|x0:i , rationale (x0:i))] Note that, in principle, this provides no advantage over an optimal language model that already correctly models the languages distribution over strings. Yet, in practice, extensive prior work has shown that language models benefit from intermediate rationales on reasoning tasks (Nye et al., 2021; Zelikman et al., 2022; Wei et al., 2022b). Some work has aimed to explain the effects of chain-of-thought reasoning, namely attributing it to locality of experience (Prystawski et al., 2024). More broadly, reasoning allows a model to decompose a challenging computation into smaller steps. In effect, we train the model to learn which decomposition and planning steps are effective in predicting future text. Also note that we formulate the objective as accurately predicting the remaining sequence, rather than only the next token. Once again, for an optimal LM these would be equivalent. However we find that the non-myopic formulation leads to a more effective loss for learning rationales. 4 Quiet-STaR 4.1 Overview Quiet-STaR operates with three main steps (Figure 1): 1. Parallel rationale generation (think, Subsection 4.2): In parallel across n tokens xi in an input sequence x0:n, we generate r rationales of length t: ci = (ci1 , . . . , cit), resulting in n  r rationale candidates. We insert learned &lt;|startofthought|&gt; and &lt;|endofthought|&gt; tokens to mark each rationales start and end. 2. Mixing post-rationale and base predictions (talk, Subsection 4.3): From the hidden state output after each rationale, we train a mixing head  a shallow MLP producing a weight determining how much the post-rationale next-token predicted logits should be incorporated compared to the base language model predicted logits. This approach eases distribution shift early in finetuning, due to introducing rationales. 3. Optimizing rationale generation (learn, Subsection 4.4): We optimize the rationale generation parameters (start/end tokens and LM weights) to increase the likelihood of rationales that make future text more probable. We use REINFORCE to provide a learning signal to rationales based on their impact on future-token prediction. To reduce variance, we apply a teacher-forcing trick to include in the loss the likelihood of predicting not only the token after the thought but also later tokens. 4.2 Parallel Generation A key challenge in Quiet-STaR is efficiently generating rationales at each token position in the input sequence. Naively, this would require a separate forward pass for each token, which becomes computationally intractable for long sequences. We allow for highly parallel generation by first observing that an inference pass of a language model produces a probability distribution over the next tokens for all input tokens. Naturally, this allows us to sample one next token from each token in the input. If one has generated a successor from each token, it is not possible to simply continue with the original sequence. For example, imagine predicting the next token after each token of &lt; bos &gt; the cat sat one might generate yes orange saw down  each successor by itself is a reasonable next token 5 a b c d a b c d Base a b c d Text Thought Token 1 Thought Token 2 a b c d a b c d a b c d a b c d Base Text Thought Token 1 Generation Paths Parallel Inference Mask Figure 3: Parallel Generation. By constructing an attention mask that allows all thought tokens to pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text, we can generate continuations of all of the thoughts in parallel. Each inference call is used to generate one additional thought token for all text tokens. to a prefix of the sequence, but the list of tokens is a set of counterfactual continuations of these prefixes. We can, however, leverage these continuations to generate hidden thoughts for each observed token. To do this efficiently, we cache each forward pass and concatenate a diagonal attention mask to the previous attention mask: each generated token now attends to all of the tokens that were used to generate it, as well as to itself (but not to token on other counterfactual paths). Moreover, this parallelized next-sampling token procedure can be repeated arbitrarily many times (or at least, until one runs out of memory). We visualize this procedure in Figure 3 and highlight additional ways to make this algorithm faster in Appendix B. 4.3 Mixing (Residual) Heads When starting with a pre-trained model, thoughts will initially be out of distribution, and hence harm language modeling performance. To smooth the transition to thinking, we introduce a learned interpolation between the LM predictions with and without thoughts. Given the end-of-thought tokens hidden state and the hidden state of the original text token, the mixing head outputs a weight that determines the extent to which the postthought prediction logits will be used. We use a shallow multi-layer perceptron for this head, outputting a scalar for each token. We include implementation details in Appendix A. 4.4 Optimizing Rationale Generation 4.4.1 Optimizing Start-of-Thought and End-of-Thought Tokens The &lt;|startofthought|&gt; and &lt;|endofthought|&gt; tokens serve as learned meta-tokens that control the models rationale generation. Optimizing the representation of these tokens, especially the &lt;|startofthought|&gt; token, is crucial but challenging due to the discrete nature of the rationale tokens. We initialize the start and end token embeddings to the embedding corresponding to the em dash, , which often appears in text data to denote a pause or thought. This leverages the language models preexisting knowledge. In addition, to allow these embeddings to be optimized more quickly, we apply a (hyperparameter) weight to the gradients of these embeddings during the update step. Intuitively, the start thought tokens can be understood as putting the model into a thinking mode and the end thought token can be understood as telling the model when its done thinking. 4.4.2 Non-myopic Scoring and Teacher-forcing Because we do not expect thoughts to be useful in predicting every token, we would prefer the models reward to depend less on the exact next word in the text following the thought and more on the following semantic content. There are two primary challenges here. First, unlike in typical language modeling with transformers, only the thoughts corresponding to 6 d e f g h &lt;t&gt; Thought &lt;/t&gt; g h Figure 4: Forward Pass and Teacher Forcing. We visualize a single forward pass of our algorithm. Solid lines denote language model computation, while dashed lines indicate tokens are inserted via teacher forcing, and the mixer represents the mixing head. In particular, we visualize predicting three tokens ahead. Thought generation is shown in more detail in Figure 1 and Figure 3. a given next-token prediction receive a gradient from that predictiona consequence of our parallel sampling strategy. We could address this by adding loss terms for future tokens by sampling the tokens before. However this would result in much higher entropy for language modeling in general and lower-quality generated text, because it would train the LM to partially disregard its preceding tokens. Instead, we use the parallel attention mask to compute the log probabilities of the true next tokens, applying teacher forcing by assuming the model selected the correct next ground-truth token (as implicit in normal language modeling with transformers). Note that the loss for each future token also depends on a mixing weight computed from the end thought token and the previous observed token. The number of future tokens included in the loss is a hyper-parameter. We apply the same teacher-forcing technique to insert the start and end tokens. We visualize this procedure in Figure 4. 4.4.3 Objective We use REINFORCE to optimize the likelihoods of the rationales based on their usefullness: the log-likelihood of the ntrue true next tokens Xj+1:j+ntrue+1 under the language model given previous observed tokens and a particular rationale (p talk j:j+ntrue as shorthand for the mixed prediction probabilities after thinking, see Algorithm 1). To reduce variance, we generate multiple rationale continuations for each token in the input sequence (loosely inspired by TRICE, Phan et al. (2023)). We thus define the reward rj for each rationale Tj as the difference between p talk j:j+ntrue and the average across rationales for that token (p talk j:j+ntrue ): rj = log p talk j:j+ntrue (Xj+1:j+ntrue+1)  log p talk j:j+ntrue (Xj+1:j+ntrue+1) We then use this reward in a REINFORCE loss term to update the language model parameters  to increase the likelihood of rationales that perform better than the average: L REINFORCE j = rj   log p (Tj |[X:j ; &lt;|startofthought|&gt;]) We found it useful to exclude the negative reward from the REINFORCE loss term, as it led to more stable training, though it may introduce some bias. This loss term encourages the model to generate rationales that improve its predictions of future tokens compared to the average prediction across all generated rationales for that token. The gradients from this loss are used to update both the LM parameters and the start-of-thought and end-of-thought token embeddings, with a (hyperparameter) weight applied to the gradients of the start-of-thought and end-of-thought token embeddings to accelerate their optimization. By iteratively optimizing these parameters, Quiet-STaR trains the model to generate more useful rationales throughout training. Lastly, we also include a log-likelihood loss, L NLL j , to ensure that the LM learns to optimize the talking heads and also receives a next-token prediction signal for the base LM head1 . 1Due to our linear mixing, equivalent to shifting the mixing weight toward the base prediction. 7 5 Experiments and Results Intuitively, not all tokens require equal amounts of thought. For example, consider the sentence the person is run-: although there is inevitably some probability of the token being something other than ing2 , as a standalone sentence without context, additional thinking is unlikely to improve a well-trained models prediction. Indeed, we conjecture that for most chunks of most online text, additional thought has little to no impact. Indeed, early in our exploration we observed that Quiet-STaR does not benefit all tokens equally. Thus, we design our experiments to investigate whether our approach is useful in predicting tokens that do require thought. We evaluate 1) whether Quiet-STaR improves a language models ability to directly predict answers in datasets that require reasoning; and, 2) the distribution of impacts resulting from thinking tokens. We conduct all of our experiments starting with the base version of Mistral 7B (Jiang et al., 2023). We perform most of our experiments by training on OpenWebMath (Paster et al., 2023), a crawl that emphasizes more technical webpages. We selected OpenWebMath because we anticipated that it would have a higher density of tokens that benefit from reasoning, which our experiments support. We also evaluate Quiet-STaR on C4 (Raffel et al., 2020), a widely used LM pretraining corpus with more diverse text, and again show significant albeit smaller benefits. 5.1 Downstream Performance In this subsection, we evaluate the extent to which Quiet-STaR improves the zero-shot reasoning capabilities of the language model on CommonsenseQA (Talmor et al., 2018) and GSM8K (Cobbe et al., 2021). On CommonsenseQA, we find that Quiet-STaR improves performance by 10.9% compared to the base language model. As shown in Figure 2, this improvement consistently increases with the number of tokens used in the models rationales, indicating that more thorough reasoning through the thought tokens is translating to better direct question-answering performance. Similarly, on GSM8K, Quiet-STaR results in a 5.0% boost over the base model, and once again, performance scales with the length of the rationales generated during Quiet-STaR training. For reference, in Figure 2, we include a baseline corresponding to training the same model on the same dataset without thought tokens. We observe that in multiple curves, performance appears to eventually deteriorate  we anticipate that this is because we are not training on these downstream tasks, so the roles of the thought tokens may change over time. We also find a benefit of our non-myopic objective, which we discuss in Appendix D. We find that training with Quiet-STaR on C4 (Raffel et al., 2020) also improves performance on GSM8K (5.9%  8.1%) and CommonsenseQA (36.3%  42.6%) but by a smaller margin. Specifically, for our C4 evaluation, we train Mistral 7B with 16 thought tokens and 4 true tokens ahead and otherwise the same setup. We can compare these improvements to those offered by pause tokens (Goyal et al., 2023), which can be seen as a constrained version of Quiet-STaR where each token is represented by two tokens and the second pause token acts as the entirety of the thought. In particular, our setup is most comparable to their pause token fine-tuning, as we also finetune a pretrained model. Their results indicate that pause token fine-tuning also provides minor gains over the base model on CommonsenseQA, they observed an improvement from 26.9% to 28.8%; on GSM8K, Goyal et al. (2023) found that pause token fine-tuning harms performance. Moreover, on both tasks (and the majority of their evaluated tasks), they observed that additional thought tokens harmed performance. Moreover, they discuss the lukewarm effect of pause-finetuning a standard-pretrained model (Goyal et al., 2023). This suggests that allowing the model to generate multi-token rationales leads to more effective reasoning compared to the single-token pauses. Note however, that unlike Goyal et al. (2023), we do not fine-tune on the downstream tasks. Overall, these downstream results validate that training a language model to predict the subtext between the lines of general text data can substantially improve its reasoning 2For example, in this very text, the token following run is - 8 1 2 3 4 5 6 7 8 Samples for Majority Vote 30 40 50 GSM8K Accuracy (%) Accuracy on GSM8K vs Number of Majority Vote Samples Quiet-STaR + CoT Baseline (CoT) Figure 5: Zero-shot performance on Quiet-STaR applied to chain-of-thought on GSM8K. We visualize how using a Quiet-STaR trained Mistral model can improve chain-of-thought performance. We use an 8-thought-token-trained model and use its internal thoughts to improve the tokens in a zero-shot chain-of-thought (Kojima et al., 2022) capabilities, even on datasets it was not explicitly trained on. The fact that longer rationales consistently lead to better outcomes, and that Quiet-STaR outperforms the constrained pause token approach, supports the notion that Quiet-STaR is successfully teaching the model to leverage its own generated thoughts to reason more thoroughly about the input. 5.2 Improvement Distribution As visualized in Appendix Figure 7, we find that on average there is little improvement in the LMs ability to predict arbitrary tokens. But, when we visualize the distribution of relative improvements, there is a disproportionate improvement on more difficult tokens. This reflects the idea that some text tokens are substantially harder and benefit more from careful thought. In Appendix Figure 8, we aim to provide some insight into the kinds of tokens where the improvements occur. Namely, while thinking appears to help for many tokens in the example, inspection suggests it disproportionately help to predict tokens where recalling relevant information is useful, such as the name of an applicable theorem or the start of the next step in a proof. Notably, this would align well with the framing proposed by Prystawski et al. (2024). 5.3 Quiet-STaR and Chain-of-Thought While there are natural parallels between chain-of-thought prompting and our approach, they are orthogonal and complementary. In zero-shot chain-of-thought, a user actively prompts the model to think out loud, otherwise using its ordinary production distribution (Kojima et al., 2022); Quiet-STaR instead allows a model to think quietly at every token, with a distribution trained to be useful. We investigate using silent, Quiet-STaR, rationales while generating explicit CoT reasoning. Because our goal is generalist reasoning that requires no task-specific input at all, we used a zero-shot prompt (Lets think step by step.) without in-context examples. Our experiments indicate that internal rationales allow the model to generate more structured and coherent chains of thought, shown in Appendix E and visualized in Figure 5. The majority vote accuracy over 8 samples (cot-maj@8) increases from 40.6% to 47.7% with Quiet-STaR, as evaluated on a sample of 128 GSM8K test items. Note that each chain-of-thought solution is sampled with temperature 0.7. 5.4 Examples While there is no explicit regularization in Quiet-STaR for thoughts to be humaninterpretable, they are generated from the same transformer trained to model language, hence likely to be at least partially understandable. We discuss why this design choice benefits the training stability in Appendix I. For reference, we include examples of thoughts generated that were helpful to the model in predicting future tokens in OpenWebMath. First, in one case, recalling that one should start with magnesium to produce magnesium nitride allows it to better predict that the first step of the procedure involves heating magnesium. 9 &#39;&lt;s&gt; # Magnesium reacts with nitrogen to form magnesium nitride. The chemical formula for this reaction is Mg+N_2-&gt; MgN_2. What is the product, or what are the products, of this reaction?nnJan 12, 2016nnThe formula for magnesium nitride is M {g}_{3} {N}_{2}.nn Explanation:nnAs do many active metals, magnesium nitride can be&lt;|startofthought|&gt; 1 --, so the equation of the reaction that forms magnesium nitride isnnMg + N_2 to &lt;|endofthought|&gt; formed by heating the metal (fier&#39; In some cases, the most useful thoughts appear to be near-continuations that correspond more closely to the target text, e.g., An integer n is odd if n = 2k+1 for some integer k.nnTo prove that A = B, we must show that A subseteq B and B subseteq A. The first of these tends to&lt;|startthought|&gt; in some sense - to be the more difficult&lt;| endthought|&gt; trickiest for students Lastly, we include an example from answering CommonsenseQA. Notably, this thought occurs while reading the question and hence was not used to predict the final answer. &#39;&lt;s&gt; Q: Talking to the same person about the same thing over and over again is &lt;|startofthought|&gt;nn(a) a one-to-one correlationnn(b) a one-to&lt;| endofthought|&gt; something someone can what?&#39; 6 Limitations This work proposes a new framework for learning to reason, and in doing so explores solutions to a variety of meta-learning challenges. However, to solve these challenges, certain simplifications were necessary. For example, it would be valuable to understand whether these techniques work when a model is trained from scratch. We have also only applied Quiet-STaR to a 7 billion parameter model, albeit a powerful one. The same techniques applied to a better model would likely yield disproportionately better results, as has often been observed for gains from reasoning (Wei et al., 2022a). Quiet-STaR results in a substantial overhead, generating many tokens before generating every additional token. (See Appendix C for compute adjusted performance results.) However, this can also be seen as an advantage: typically, a language model can generate the next token based on the current context, and while there are techniques to improve sampling quality, there is no general way to leverage additional compute to enhance next-token prediction. In the current implementation we do not support dynamically predicting when to generate, or end, a rationale. However, this would be a natural extension. For instance, if the mixing head was a prediction from the base language model, before any thought, rather than after the thought, one could apply a threshold to prevent generating thoughts that would not be incorporated. We expect that this is a more difficult task, as predicting the usefulness of a thought is simpler when one has already generated the thought. 7 Conclusion Quiet-STaR represents a step towards language models that can learn to reason in a general and scalable way. By training on the rich spectrum of reasoning tasks implicit in diverse web text, rather than narrowly specializing for particular datasets, Quiet-STaR points the way to more robust and adaptable language models. Our results demonstrate the promise of this approach, with Quiet-STaR improving downstream reasoning performance while generating qualitatively meaningful rationales. We believe this also opens many potential future directions - for example, one may aim to ensemble thoughts in order to further improve the predictions for future tokens. Moreover, if the language model can predict when thought will be useful, for example by putting the mixing head before the prediction, then the predicted mixing weight could be used to dynamically allocate compute during generation. Future work can build on these insights to further close the gap between language model and human-like reasoning capabilities. 10 Ethics Statement This work raises some important ethical questions, many of which also apply to STaR. For example, it is impossible to know that the reasoning expressed by the model in language accurately represents the internal processing of the model (i.e., faithfulness). In addition, regardless of faithfulness, there are no safeguards against harmful or biased reasoning patterns if the model finds them useful. Relatedly, we note that CommonsenseQA is known to have many biased questions and low-quality answers (Geva et al., 2019), but we use it in line with prior work (Zelikman et al., 2022; Goyal et al., 2023). Thus, aside from improving language modeling, it is unclear in what capacity the rationales themselves should be used. Acknowledgements We particularly thank Xindi Wu, Michael Li, and Qian Huang for their helpful and detailed comments, as well as Xuechen Li, Jan-Philipp Franken, Yuhuai Wu, Gabriel Poesia, Winnie  Xu, Omar Shaikh, Fan-Yun Sun, Joy He-Yueya, Omar Khattab, and William Yin for useful discussions. In addition, we would like to acknowledge that this work was supported by NSF Grant #2302701. References Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. Advances in neural information processing systems, 30, 2017. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416, 2022. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems. arXiv, 2021. eprint: 2110.14168. Kanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic reasoning with language models. arXiv preprint arXiv:2305.19165, 2023. Mor Geva, Yoav Goldberg, and Jonathan Berant. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. arXiv preprint arXiv:1908.07898, 2019. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno,  Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=SaRj2ka1XZ3. 11 John Hewitt, John Thickstun, Christopher D Manning, and Percy Liang. Backpack language models. arXiv preprint arXiv:2305.16765, 2023. Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022. Matthew Douglas Hoffman, Du Phan, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A Saurous. Training chain-of-thought via latent-variable inference. Advances in Neural Information Processing Systems, 36, 2024. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Hoyoun Jung and Kyung-Joong Kim. Discrete prompt compression with reinforcement learning. arXiv preprint arXiv:2308.08758, 2023. Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp. arXiv preprint arXiv:2212.14024, 2022. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. Takeshi Kojima, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners, 2022. URL https://arxiv.org/abs/ 2205.11916. Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. Jack Lanchantin, Shubham Toshniwal, Jason Weston, Sainbayar Sukhbaatar, et al. Learning to reason and memorize with self-notes. Advances in Neural Information Processing Systems, 36, 2024. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. 12 Michael Y Li, Emily B Fox, and Noah D Goodman. Automated statistical model discovery with language models. arXiv preprint arXiv:2402.17879, 2024. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models. arXiv preprint arXiv:2310.06201, 2023. Jiacheng Liu, Ramakanth Pasunuru, Hannaneh Hajishirzi, Yejin Choi, and Asli Celikyilmaz. Crystal: Introspective reasoners reinforced with self-feedback. arXiv preprint arXiv:2310.04921, 2023. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self. Feedback, 2023. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 19281937. PMLR, 2016. Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36, 2024. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. Alexander Pan, Erik Jones, Meena Jagadeesan, and Jacob Steinhardt. Feedback loops with language models drive in-context reward hacking. arXiv preprint arXiv:2402.06627, 2024. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. Du Phan, Matthew Douglas Hoffman, Sholto Douglas, Tuan Anh Le, Aaron T Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, Rif A Saurous, et al. Training chain-of-thought via latent-variable inference. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Gabriel Poesia, Kanishk Gandhi, Eric Zelikman, and Noah D Goodman. Certified reasoning with language models. arXiv preprint arXiv:2306.04031, 2023. Stanislas Polu and Ilya Sutskever. Generative Language Modeling for Automated Theorem Proving. CoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393. eprint: 2009.03393. Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning emerges from the locality of experience. Advances in Neural Information Processing Systems, 36, 2024. 13 Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 49324942, 2019. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Tal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai. Programming Puzzles. In Thirty-fifth Conference on Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=fe hCc4RBrg. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023. Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 46154629, 2020. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. Neural Information Processing Systems (NeurIPS 2022) Workshop on MATH-AI, 2022. Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023. Xuezhi Wang and &lt;PRESIDIO_ANONYMIZED_PERSON&gt;. Chain-of-thought reasoning without prompting. arXiv preprint arXiv:2402.10200, 2024. 14 Lucas Weber, Jaap Jumelet, Elia Bruni, and Dieuwke Hupkes. Language modelling as a multi-task problem. arXiv preprint arXiv:2101.11287, 2021. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021a. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021b. Jason Wei, Yi Tay, Rishi Bommasani, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models, October 2022a. URL http://arxiv.org/abs/2206.07682. arXiv:2206.07682 [cs]. Jason Wei, Xuezhi Wang, &lt;PRESIDIO_ANONYMIZED_PERSON&gt;, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models, 2022b. URL https://arxiv.org/abs/2201.11903. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. International Conference on Learning Representations (ICLR 2023), 2022. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, and Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions, 2023a. Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop): Recursively self-improving code generation. arXiv preprint arXiv:2310.02304, 2023b. Hugh Zhang and David C Parkes. Chain-of-thought reasoning is a policy improvement operator. arXiv preprint arXiv:2309.08589, 2023. Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon. In-context principle learning from mistakes. arXiv preprint arXiv:2402.05403, 2024. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022. Wenting Zhao, Justin T Chiu, Claire Cardie, and Alexander M Rush. Hop, union, generate: Explainable multi-hop reasoning without rationale supervision. arXiv preprint arXiv:2305.14237, 2023. Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022. Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, and Hanjun Dai. Large language models can learn rules. arXiv preprint arXiv:2310.07064, 2023. 15 Appendix A Hyperparameter Choices Optimization and Evaluation For optimization, we use the AdamW optimizer with a warmup of 20 steps, a learning rate of 1e  6, a weight decay of 0.001, and a batch size of 8 (along with any necessary gradient accumulation to keep this fixed across runs). Moreover, our &lt;|startofthought|&gt; and &lt;|endofthought|&gt; embedding gradient weight is 1e2 and our policy weight is 1e6. We sample with temperature T = 1 during training and use greedy decoding for the thoughts during evaluation. We treat our samples as importance samples by computing the REINFORCE loss at temperature T = 3. Because we do not prompt the model with any examples, we directly compute the probability of the correct answer, conditioned on generating an answer  for example, for multiple choice questions between A    E, we compute the accuracy over the logits for tokens corresponding to A    E. Lastly, for our training, we select a random span of 256 tokens from each sample (or pad if there are fewer than 256 tokens). Mixing Head For our mixing head, we use a three-layer MLP with ReLU activation, taking in a vector of two times the size of the hidden state of the language model (as we concatenate the two predictions to determine their weights), and outputting a scalar. This scalar is them used to weight the logits from the LM head with and without thinking to make a prediction from a given token. Computation We train all of our models on a single node of eight 80GB H100s. B Faster Parallel Sampling In this section, we highlight some simple ways to further accelerate the parallel generation algorithm. For example, note that one can reduce the attentions memory cost by computing the diagonal attention simply as elementwise (rather than pairwise) dot-products. That is, given two input embedding sequences of shapes (b, t, l, d) and (b, 1, l, d) where t is the number of timesteps ahead, b is batch size, l is sequence length, and d is embedding dimension, we do not need to compute their pairwise attention of shape (b, t, l, l), we only need to compute the attention for the paired elements along the diagonal of shape (b, t, l). Additionally, to avoid generating continuations for all of the tokens (for example, if one wanted to apply a value function to determine where thoughts would be most useful), one can index into this generated attention mask. Notably, however, this also requires manipulation of the other inputs during the forward pass such as the positional embeddings. C Compute-Adjusted Plots We also visualize Figure 2 where we normalize by the number of thought and talk tokens used for training. D Measuring the Impact of Multiple Thoughts Per Sequence and Multiple Tokens Ahead We perform a simple ablation on our 12-thought-token-4-ahead baseline, namely asking whether sampling multiple thoughts per sequence is necessary. We find that although simply computing the reward as the difference between the losses with and without thought proves to be a strong baseline, using multiple thoughts consistently outperformed it (by roughly 0.5% on GSM8K generalization and 3% on CommonsenseQA generalization). However, the exact number of thoughts had little impact: varying between 2, 3, and 4 thoughts per sequence appeared to result in a consistent improvement with additional 16 500 1000 1500 2000 2500 3000 3500 Training Step 6% 7% 8% 9% 10% 11% GSM8K Accuracy (%) # Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (a) GSM8K 500 1000 1500 2000 2500 3000 3500 Training Step 34% 36% 38% 40% 42% 44% 46% CommonsenseQA Accuracy (%) # Thought, Ahead Tokens 24, 12 16, 8 12, 4 10, 4 8, 4 Baseline (b) CommonsenseQA Figure 6: Compute-Normalized Generalization Results. We visualize the performance curves normalized by the number of inference calls used. thoughts, but additional thoughts per sequence beyond two improved performance by only 0.1-0.3% on both GSM8K and CommonsenseQA. Similarly, we found that predicting more than one token ahead helped by 0.3% on GSM8K and 3.1% on CommonsenseQA, but with 12-thought-tokens, we did not find that additional tokens ahead, beyond two, improved performance. However, qualitatively, we found that the rationales appeared more coherent with additional tokens-ahead of supervision. This suggests a need for more specific evaluations of generation quality in future work. E Rationale-Augmented Rationales Because the language model can insert thoughts anywhere, we perform a preliminary study where we investigate the reasoning generated by the language model when it has access to internal rationales while it generates external rationales (i.e. chains-of-thought) to solve a problem. Because these chains of thought are themselves generated by chains of thought, one might refer to this as a net-of-thought (or NoT). We compare five solutions from the model to a fixed random question in GSM8K, looking at outputs of the base Mistral 7B, the outputs of Mistral fine-tuned for the same number of steps on OpenWebMath (Paster et al., 2023), and the rationales of our best Quiet-STaR model. We sample from all of them directly, at temperature t = 1. E.1 Base Mistral 7B First Response: A: Let&#39;s think step by step. How many fresh eggs remains after the three her eat for breakfast and four goes into the muffin mix for her friends. We can calculate it by 16-3-4. The amount equals 9 fresh eggs. So, the dollars it makes daily equals what multiplied by 2. 12 x 2. The answer is 24. Second Response: Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. She eats 3 daily for breakfast. She bakes 4 daily into muffins for her friends. That leaves 9. She sells 9 for (2), so (9times 2=18 ). So Janet makes 18 selling duck eggs each day. 17 66. Janet sells an average of 12 fresh duck eggs daily on the farmers&#39; market. If she sells them for 2 per egg how much does she make per week, assuming she sells at the farmers&#39; market most every day?A: If Janet sells on average 12 fresh duck eggs daily on the farmers&#39; market. This is equal to (12times 7), so 12 (times) 7 =(84). The total amount she would then make would be (84times 2) dollars, or 168 per week. So Janet makes 168 selling duck eggs per week. Third Response: Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. How many eggs does she have left every day? This is important because it will help us find how many eggs she sells each day. What can we do to answer the question? If we write the relationship between how many duck eggs Janet has left, how many she eats, and how many she bakes for the muffins, then we can see how many eggs she has left to sell at the farmers&#39; market. Let&#39;s write an equation to solve our dilemma. Subtract an equation for eggs eaten and an equation for eggs baked to find the eggs remaining. Let&#39;s see what we can find. We can see that Janet has 6 eggs leftover each day. 6times2 = 12 per day. Therefore Janet receives an additional 12 per day for her eggs at the farmer&#39;s market. Remember, this is just for one day, so now she makes 12 per day in additional income, because she sells her eggs at the farmer&#39;s market each day. Answer 12 Fourth Response: Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. First, you want to find out the number of eggs a total that Janet collects each day. Take this many dollars a three and subtract from the money she gets per egg, which is 2. The answer to that question is 5. Fifth Response Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. For the first step, let&#39;s think of the day as a whole, using some variables to keep track of the numbers of eggs. Let n = the number of eggs that Janet&#39;s ducks lay per day. Then the number of eggs that she eats per day will be (3) = . Next, she makes m = muffins. For each muffin, she uses 4 = eggs, so we can take from the eggs Janet&#39;s ducks lay to represent the number she uses: 4(m)=. Now, to get her back to our starting number of eggs, we need to add these eggs back on, giving us . The eggs Janet sells each day brings her 2(n) = . This brings us to our next step. Now we will break down all the eggs from our starting number, n =, into the pieces that we set up previously. For the beginning number, we have: This brings us back to our starting number, and we can simplify this expression. 18 Moving from the top line to the second line, we multiply the numbers within each parenthetical group. Moving from the second line to the third line, we add the products together to find the total. Finally, we can distribute and determine the final product: = 360. So the total number of eggs that Janet&#39;s ducks lay per day is 360. Since Janet sells each egg she has for 2, her total earnings at the farmers&#39; market is: boxed{ 720}. E.2 OpenWebMath-Trained Mistral 7B First Response: &lt;s&gt; Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. How much does she make every day? Well per the question, that is 2 per a duck egg. Now, how many duck eggs per day does she sell? Well, that&#39;s 16 minus the eggs she eats minus the eggs she uses to ma ke muffins. She eats 3 eggs every morning, so 16-3 = 13. She makes 4 muffins, which uses 4 eggs. 16-3-4= 5. Therefore, the number of duck eggs she is selling is 5. Clearly, 1 duck egg costs 2. Therefore, the total amount she makes selling the eggs is 10 2. Now, this can be written as a fraction as, &quot;ten two-dollars,&quot; so 10 2 - bucks. Converting to a fraction, or &quot;a tenth of ten,&quot; so 10 2 - bucks = 1 10 - bucks [reveal-answer q=&quot;fs-id1165135856725&quot;]Show Solution[/reveal-answer] [hidden-answer a=&quot;fs-id1165135856725&quot;] Second Response: &lt;s&gt; Q: J a n e t s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. On what basis can we answer the question affer considering the question? What do we know given and how can we use what we&#39;re given to arrive at an answer. 3 is given, 4 is given, 16 is given, does anyone have any thoughts? A: Well 16 minus 3 minus 4 equals 9, and then I subtract 1 from 9 and that is 4 extra so five more eggs. M: And we know that 2 times 5 is how much? A: Ten M: Yes. A: If she eats 3 duck eggs for breakfast, in the morning equals (3 times 2 = 6). If she eat 4 duck eggs baking muffin for her friends equals (4 times 2 = 8), so total of money and that she got is 6 plus 8 which equals 14. The weight gain is 5 because it is 5 is that what you put up there so 5 times 2 ducks and subtract that from that from 16 and you got 7 no what did we write down? Are you able to figure out now what the question is, and what is what the question really is. You got 14 the weight loss you, you subtracted that from 16 which equals 2, 14 minus 2, remainder is what? A: 12. M: The question is what is J a n e t s ducks lay 16 eggs, she eats 12 for sale. You subtract 3 plus 4 minus 5 so 5 when you eat the muffin equals 5, times two, the 12, minus the money so 5 times 2, -12. So (2 times 9 = 18) minus 10, so 4 more eggs... Still times 2, total comes to to 20. A: I understand there is the ones day one 3 plus 4 eats who makes and so on and so on. M: But maybe in order to answer it correctly you have to look a little close. Typically in the procedures you have that they give you first, I can tell you how here we go. 19 M: Now if you increase the 16, now you have that you would need to determine 16 minus 3 minus 4 and that adds up to the remainder, which is 9. Then subtract 1 and but you have 5. The next answer then, which you do is now 5 times 2, which is 10. M: Which is 10, and then 14, which is is the remainder M: Is 6 and 8 and that is a total of 14, and now that you subtract 2, and that means that you now have 16 minus 5, which is 11. M: The next thing is to the yield for the second situation you have got 16 minus 3 plus 4 and that comes to 9. This is very easy to do instantaneously even the first part, 16, 3, 4 and 9, and you happen to know what lives, and you just count and you have five and five then go ahead. 5 times 2 comes to 10, so 10 then so actually you have just added 4.5, now your total is 16 plus 2. But now to this is 10 plus 2 is 12, so how much you sell from 16 and then minus 12 comes to 4. M: So what is the remainder of 16 minus 3 plus 4 minus 5? Matters you see three is missing. Four you have. Five eggs. So 5 times 2, so 10 remains at 10, so reduce, so what happens now to what happened was added with subtracted the next the question is 16 minuses 3 plus 4, and presumably you&#39;ve done this before, if not then you add up 16 minus 3 plus 4 gives us 9 and what is the remainder after 5? So what is 5 times 2? So 10. 2, 3, plus 2 = 5 , and so your answer so far is 16 minus 3 plus 4, and give us the basket minus 2 remains 9 and now you have to determine now what is the total for the second part? You have Third Response: &lt;s&gt; Q: J a n e t s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. The &quot;how much in dollars&quot; part means that one egg is worth one dollar. So, twelve ducks lay 16 eggs, which means that Janet nets 20 from selling them. 16 muffins will give us 4 eggs, which nets us 8. So, we paid 108 for our ingredients to make 16 muffins. Hence, we lost 96 dollars. Hello, heroine, we have to do better than that! Look into averages. Let&#39;s say that we pay 5 dollars for one egg. Now, 12 is our product and 5 is our investment. Our net wealth will be 12-5*12 = 24 dollars. Is that correct? Fourth Response: &lt;s&gt; Q: J a n e t s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. First, Jane&#39;s ducks lay 16 eggs perday. So there are 16 eggs Monday, 16 Tuesday, 16 Wednesday, 16 Thursday, 16 Friday, and so on. Second, Janet eats 3 of the 16 eggs that the ducklay ap day for breakfast. She only eats the duck eggs, not her baked goods (muffins) or other food. Therefore, she eats 3 Monday, 3 Tuesday, 3 Wednesday, 3 Thursday, and so on. Third, she only bakes the first 4 non-eaten eggs into muffins. Therefore she sells the remaining 12 noneaten eggs Monday, 12 Tuesday, 12 Wednesday, 12 Thursday, and so on. Fourth, Janet sells duck eggs Monday, Tuesday, Wednesday, Thursday, Friday at the 2 each. Third, she eats 3 Monday, 3 Tuesday, 3 Wednesday, 3 Thursday, Friday at the farmers&#39; market. Lastly, she makes 12 Monday, 12 Tuesday, 12 Wednesday, 12 Thursday, and so on in dollars every day at the farmers&#39; market. The worksheet below summarizes these steps. The number formed by the intersection of corresponding row and column terms will be the correct answer. The product of the (3, J) term and the (J, 8) term is 1,150. So, Janet makes 1,150 per day! (Note: Step 3 and Step 8 are repeating, but the (3,7) term in Step 8 is not the same as the (3,7) term in Step 3. ) Fifth Response &lt;s&gt; Q: J a n e t s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at 20 the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. What do we know? - Janet&#39;s ducks lay 16 eggs per day. - She eats three for breakfast every morning and bakes muffins for her friends every day with four. From these two facts, we can conclude at least that Janet needs at least seven eggs for consumption per day. And what do we need to find exactly? - We need to find how much in dollars Janet makes every day at the farmers&#39; market. Let&#39;s write an equation and solve it to find the answer. We know: - (16-3-4=9) So our daily profit is 9 times 2, or (9times2=18) Hence her profits at market on a given day are (18). Answer: (18). E.3 Quiet-STaR-Trained Mistral 7B First Response: &lt;s&gt; Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. The ducks lay 16 eggs per day. This is the total number of eggs. 3 of the eggs are eaten for breakfast. This is the number of eggs that are eaten for breakfast. 4 of the eggs are used to make muffins. This is the number of eggs that are used to make muffins. 16 - 3 - 4 = 9 9 is the number of eggs that are sold at the farmers&#39; market. 9 of the eggs are sold at the farmers&#39; market. 2 The price of each egg is 2. 2 times 9 = 18 18 is the total price of the eggs. The total price of the eggs is 18. Second Response: 21 A: Let&#39;s think step by step. 1. The ducks lay 16 eggs per day. 2. She eats 3 for breakfast every morning. 3. She bakes muffins for her friends every day with 4. 4. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. 5. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 6. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 7. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 8. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 9. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 10. The number of eggs she sells at the farmers&#39; market is 16 - 3 - 4. 11. The number of Third Response: &lt;s&gt; Q: Janet&#39;s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers&#39; market daily for 2 per fresh duck egg. How much in dollars does she make every day at the farmers&#39; market? A: Let&#39;s think step by step. The ducks lay 16 eggs per day. This is the total number of eggs. 3 is the number of eggs that Janet eats for breakfast every morning. 4 is the number of eggs that she uses in the muffins that she bakes for her friends every day. 16 - 3 - 4 = 9 9 is the number of eggs that she sells at the farmers&#39; market daily for 2 per fresh duck egg. 9 2 = 9 2 = 9 2 = 9 2 = 18 So, 18 is the amount in dollars that she makes every day at the farmers&#39; market. The amount in dollars that she makes every day at the farmers&#39; market is 18. A: 18&lt;/s&gt; Fourth Response: A: Let&#39;s think step by step. The number of eggs that Janet&#39;s ducks lay per day is 16. Of these, she eats 3 for breakfast every morning. This is 3. Of these, she bakes muffins for her friends with 4. This is 4. 22 The number of eggs that are left over is 16 - 3 - 4. This is 16 - 7. This is 9. The number of eggs that she sells at the farmers&#39; market daily for 2 per egg is 9. The total in dollars that she makes every day at the farmers&#39; market is 9 2. This is 9 2. This is 18. The total in dollars that she makes every day at the farmers&#39; market is 18. A: 18 Fifth Response A: Let&#39;s think step by step. The ducks lay 16 eggs per day. This is the total number of eggs. 3 + 4 + 16 = 23 3 eggs are eaten for breakfast every morning. 4 eggs are used to make muffins for her friends every day. 16 eggs are sold at the farmers&#39; market for 2 each every day. The 16 eggs that are sold at the farmers&#39; market are the remaining eggs. 16 - 3 - 4 = 9 9 eggs are sold at the farmers&#39; market for 2 each every day. 9 (2) = 18 18 is the total in dollars that she makes every day at the farmers&#39; market. 18 is the answer. A: 18 F Additional Works on Learning to Reason For completeness, we highlight that many other works have explored teaching language models to reason (often with algorithms similar to or inspired by STaR), either from their own rationales, from interaction with an environment, or from a hand-constructed dataset. For example, works explore this in the context of multihop question answering (Zhao et al., 2023), math (Luo et al., 2023; Uesato et al., 2022), machine translation (Gulcehre et al., 2023). Several works investigate teaching language model agents to reason in planning (Chen et al., 2023; Gandhi et al., 2023; Qiao et al., 2024), or to use specific tools or memory (Yao et al., 2022; Lanchantin et al., 2024; Schick et al., 2024), while others investigate how one may distill the reasoning from a large language model into a smaller language model Ho et al. (2022); Li et al. (2022); Hsieh et al. (2023). Notably however, Pan et al. (2024) demonstrates 23 that these feedback loops may result in reward hacking. Zelikman et al. (2023b) shows how a bootstrapping loop can be implemented where a model repeatedly improves a codeimprover using the same code-improver and Haluptzok et al. (2023) shows how language models can bootstrap their programming ability with programming puzzles Schuster et al. (2021). Other works have employed a similar strategy for using language models to solve inductive reasoning tasks or to model real-world systems (Wang et al., 2023; Qiu et al., 2023; Zhu et al., 2023; Li et al., 2024). Some works have investigated how models can learn from their reasoning mistakes incontext (Shinn et al., 2023; Madaan et al., 2023; Zhang et al., 2024; Liu et al., 2023). Many studies have also focused on the ability of LMs to learn from in-context reasoning examples (Lampinen et al., 2022; Zhou et al., 2022)  correspondingly, Khattab et al. (2022) and Khattab et al. (2023) show how the sets of examples used to prompt a model to reason can be optimized in the context of a multi-step reasoning pipeline. Furthermore, Zhang et al. (2022) demonstrated that one can improve zero-shot question-answering in language models by using a variety of zero-shot prompts for reasoning. G Distribution of Improvements We also visualize the distribution of improvements across tokens in the training set. Original Token Prediction Loss Improvement in Loss Given Thought Figure 7: Distribution of changes in log probability. We visualize the distribution of changes in log probability resulting from the generated thoughts across the evaluation dataset. We visualize the density of changes in log probability relative to the LM without thoughts, colored based on log density. The distribution is skewed, with most tokens unaffected, while a small fraction of hard tokens see substantial improvements from the thoughts. This matches our intuition that most tokens in general web text do not require significant reasoning to predict, but thoughts are disproportionately beneficial for challenging tokens. 24 H Contribution Visualization 0icos^2sin^3+5cossin^4+isin^5 Then I used the Moivre&#39;s theorem and I got: (cos5+isin5) I compared the imaginary parts and I got something like: sin5=5cos^4sin-10cos^2sin^3+sin^5 which is very close to: (16cos^4-12cos^2+1)sin but not the same. Where do I make te mistake? Thanks for any help! ;)  Have you tried use cos^2theta+sin^2theta =1  Fan May 15 &#39;17 at 19:13  You didn&#39;t err. You just didn&#39;t finish. May 15 &#39;17 at 19:35 hint In your last line, factor by sin (theta), replace sin^2(theta) by 1-cos^2 (theta) Figure 8: Contribution Visualization. We provide an example text where we visualize the degree to which introducing thoughts helped at tokens throughout a text. Green indicates that the thought before that token made that token easier to predict, while yellow indicates that it made it harder to predict. More impactful thoughts have higher opacity. I Handling Instability Several aspects of this task have the potential to introduce instability. First, and perhaps most importantly, the utility of a generated thought (or thought token) is a function of the mapping from the thought to its contribution to language prediction; however, the mapping from the thoughts to this contribution is learned based on the thoughts themselves. This means that, even if one were to generate a thought that allowed the perfect prediction of the next token, the loss could receive no signal from it if the mixing heads weight on that generation was 0. One solution we explored was to use the Gumbel-Softmax trick with a straight-through estimator Jang et al. (2016), but with many consecutive softmax operations we observed vanishing gradients. This introduces an exploration-exploitation trade-off, a fundamental challenge in reinforcement learning. Approaches like DQN (Mnih et al., 2013), PPO (Schulman et al., 2017), and A3C (Mnih et al., 2016) often resolve these tradeoffs by learning a state value function, which estimates the expected future reward from each state. However, the reward functions associated with this environment are unstable (as noted earlier, due to the also-changing mixing heads)  consequently, our preliminary explorations with these techniques was not promising. While we are far from the first to note that optimizing rationales is a reinforcement-learning task (Zelikman et al., 2022; Zhang &amp; Parkes, 2023; Phan et al., 2023), the need for the rationale to avoid harming the base model performance introduces additional complexity. Essentially, the more complex the mapping from LM output to the next token prediction, the more instability we observed. On the other hand, when we trained without any interpolation, i.e. ablating the mixing head and only using the language model prediction after thoughts, the model quickly learned to simply ignore the thoughts (and we saw no generalization to any downstream tasks). We explored the use of separate heads for thinking and talking (here, we use talking to refer to directly outputting a hidden state or logits, rather than a mixing weight). In particular, we explored both linear layers from the hidden states and MLPs, initialized to contribute 0 residually to the base language model outputs, in order to generate thoughts and next-token 25 predictions similar to what the language model would have otherwise generated. However, we observed that, in all instances, the previously-mentioned instability prevented learning. Consequently, we aimed to remove or minimize all components that could transform the language models outputs, both with and without its rationales. We also note that our choice to use a language model to output a weight combining multiple states (as done by our mixing head) is essentially an attention mechanism allowing the model to attend to its thinking. This has similarity to the approach taken in Backpack language models (Hewitt et al., 2023), which also learn to predict weights to apply to summed input embeddings to model future text, rather than allowing the language model to output arbitrary embeddings. Despite this constraint, Backpack language models appear to have comparable performance to traditional language models (Hewitt et al., 2023). 26&quot;&quot;</div><div class="divider" data-v-47d7db5a></div><p class="review-content" data-v-47d7db5a><span class="icon" data-v-47d7db5a>answer:</span>To simplify this for you, the process described in this comprehensive code and explanation involves training a neural network, specifically a type of model called a transformer, to refine its capabilities to perform specific tasks or understandings without needing to train it from scratch. This is akin to teaching an existing expert (the pretrained transformer model) new tricks or refining its existing knowledge rather than training a new expert from the beginning. 1. **Choosing the Recipe Books:** The process begins by selecting a base model (like choosing a basic recipe book you know well) and a target model (a new recipe book with new recipes) from which we want to extract refined knowledge or &#39;recipes&#39;. 2. **Preparation:** Before we start, we make sure all tools (software libraries like PyTorch, transformers, etc.) are ready. 3. **Downloading the Books:** Next, we download these models (recipe books) from a large repository (Hugging Face Hub). 4. **Identifying Special Ingredients:** We then identify which parts of the model (like specific ingredients in recipes) are crucial for our new specific tasks. In modeling terms, we look for embedding and linear layers which can be adjusted or tuned. 5. **Getting the New Ingredients:** Here, we download the specific new model (or new set of recipes) we wish to learn from. 6. **Reading the Ingredients:** This step involves loading the model parameters (or, in our analogy, understanding the ingredients and methods used in the recipes). 7. **Creating a New Recipe:** In the most intricate step, we mathematically combine knowledge from both sets of recipes to create a new, simplified yet effective set of recipes (model adaptation), using a mathematical technique called low-rank decomposition. 8. **Extracting the Essence:** The new, refined knowledge or set of &#39;recipes&#39; is then saved in a simplified format so it can be more efficiently used. 9. **Testing the Dish:** To ensure the new recipes work as expected, we test them by generating responses from the model, akin to tasting a dish to ensure it has the desired flavor. 10. **Sharing Your Recipe:** Finally, satisfied with the new recipe, we share it with others by uploading it to the repository (Hugging Face Hub) for others to use. This entire process is a sophisticated way of teaching an existing neural network model new skills or refining its existing skills without needing to retrain it entirely, similar to teaching new, advanced cooking techniques to an already skilled chef.</p></div></div><div class="poem-container" data-v-91039cb8 data-v-47d7db5a><div class="review" data-v-47d7db5a><div class="review-title" data-v-47d7db5a><span class="icon" data-v-47d7db5a>question:</span>can you write code for paper S-type speed control curve based on the number of pluses</div><div class="divider" data-v-47d7db5a></div><p class="review-content" data-v-47d7db5a><span class="icon" data-v-47d7db5a>answer:</span>Certainly! S-curve speed control is commonly used in motion control applications, including robotics and CNC machines, to start and stop the motion smoothly. This approach helps in reducing the mechanical stress on parts and ensures a smoother transition across the velocity profile. To implement an S-type speed control curve based on the number of pulses (assuming the pulses correlate directly to position or speed increments), we can use a piecewise function that divides the motion into different phases: acceleration, constant speed, and deceleration. Here&#39;s a basic implementation using Python. This program will simulate an S-curve motion profile by generating the position (in pulses) over time. The S-curve in this context is generated by modulating acceleration during the acceleration and deceleration phases to have a smoother ramp-up and ramp-down. We&#39;re using a simple approach to approximate the S-curve by using a sinusoidal function to adjust the acceleration smoothly. Note: This is a simplified example. In real applications, the control parameters and algorithms can be much more complex and require tuning based on the hardware and motion requirements. ```python import numpy as np import matplotlib.pyplot as plt def generate_s_curve(total_pulses, total_time, acc_phase_fraction=0.3): &quot;&quot;&quot; Generates an S-curve profile based on total pulses and time. Parameters: - total_pulses: The total number of pulses to be generated. - total_time: The total time over which the pulses are distributed. - acc_phase_fraction: Fraction of the total time to be used for acceleration and deceleration each. Returns: - A list of pulses over the time. &quot;&quot;&quot; time_steps = np.linspace(0, total_time, num=total_time*100) # Generate 100 steps per second. pulse_profile = np.zeros_like(time_steps) acc_time = total_time * acc_phase_fraction const_time = total_time - 2 * acc_time for i, t in enumerate(time_steps): if t &lt; acc_time: # Acceleration phase # Use a sinusoidal function to ease-in acceleration. pulse_profile[i] = (total_pulses/2) * (1 - np.cos(np.pi * t / acc_time)) / (2 + const_time/acc_time) elif t &lt; acc_time + const_time: # Constant speed phase pulse_profile[i] = (total_pulses/2) * (1 + 1 * (t - acc_time) / acc_time) else: # Deceleration phase rem_time = t - acc_time - const_time pulse_profile[i] = total_pulses - (total_pulses/2) * (1 - np.cos(np.pi * rem_time / acc_time)) / (2 + const_time/acc_time) return time_steps, np.diff(pulse_profile, prepend=0) # Return the derivative as pulses per time step. # Parameters total_pulses = 10000 # Total number of pulses to reach the target total_time = 10 # Total time in seconds to reach the target time_steps, pulses = generate_s_curve(total_pulses, total_time) plt.plot(time_steps, np.cumsum(pulses)) plt.title(&#39;S-Curve Speed Profile (Total Pulses over Time)&#39;) plt.xlabel(&#39;Time (s)&#39;) plt.ylabel(&#39;Pulses&#39;) plt.grid(True) plt.show() ``` # Highlights: - This code generates a time series of pulses following an S-curve profile, which is often desired for motion control to minimize jerks. - `acc_phase_fraction` is a parameter to adjust how much of the total time is allocated to acceleration and deceleration phases. Adjusting it can control the &quot;sharpness&quot; of the S-curve. - The acceleration and deceleration phases use a cosine function to produce the S-curve&#39;s smooth gradient changes. - The constant speed phase is calculated to ensure the total pulse count matches the target. - The code uses `np.diff` with `prepend=0` to calculate the change in pulses (speed) at each time step - effectively showing how pulses are sent over time to achieve the desired motion profile. Adjust the parameters as needed for your specific application or simulation.</p></div></div><div class="poem-container" data-v-91039cb8 data-v-47d7db5a><div class="review" data-v-47d7db5a><div class="review-title" data-v-47d7db5a><span class="icon" data-v-47d7db5a>question:</span>[Response must be written as a short blurb.] Was Darius the Mede a real person?</div><div class="divider" data-v-47d7db5a></div><p class="review-content" data-v-47d7db5a><span class="icon" data-v-47d7db5a>answer:</span>The historical existence of Darius the Mede, a figure mentioned in the Book of Daniel, remains debated among scholars. Contemporary historical records and Persian sources from the period do not provide clear evidence of his reign or identity. While some suggest he could be a literary or symbolic character, or possibly conflated with other historical figures from the Medo-Persian empire, definitive evidence confirming his existence as described in biblical texts has yet to be found. Hence, the question of his real-life existence continues to spark discussion in both historical and biblical scholarship.</p></div></div><div class="poem-container" data-v-91039cb8 data-v-47d7db5a><div class="review" data-v-47d7db5a><div class="review-title" data-v-47d7db5a><span class="icon" data-v-47d7db5a>question:</span>[Response must be written as a short blurb.] What made the Parthians different from the Achaemenids and the Sassanids?</div><div class="divider" data-v-47d7db5a></div><p class="review-content" data-v-47d7db5a><span class="icon" data-v-47d7db5a>answer:</span>The Parthians, ruling from 247 BC to AD 224, were distinct from their predecessors, the Achaemenids, and successors, the Sassanids, in several key aspects. Notably, they maintained a semi-nomadic lifestyle and a feudal system of government, contrasting with the more centralized bureaucracy of the Achaemenids and the Sassanids. Their military tactics, especially their use of heavily armored cavalry and horse archers, set them apart and allowed for remarkable flexibility and effectiveness in battle. Culturally, the Parthians were more inclined towards a syncretic approach, blending Greek, Persian, and local traditions, which is reflected in their art and architecture. This multifaceted identity helped them control a vast and diverse empire but also differentiated their administrative and cultural practices significantly from those of their neighbors and successors.</p></div></div><!--]--><!----></div><button class="load-more-button" data-v-91039cb8><span data-v-91039cb8>See more</span></button></section></div></div></main><!--[--><!--]--><div class="VPFooter" data-v-b0366457><p class="license" data-v-b0366457>Released under the <a class="vt-link link link" href="https://www.m44m.com" target="_blank" rel="noopener noreferrer" data-v-b0366457><!--[-->MIT License<!--]--><!----><!----></a>.</p><p class="copyright" data-v-b0366457>Copyright  2014-2025 MIT</p></div><!--[--><!--]--></div></div><div class="visually-hidden" aria-live="polite" data-v-e4982c5a> has loaded</div></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_coc.md\":\"B3ITOP5z\",\"about_community-guide.md\":\"Dvcdb1O6\",\"about_faq.md\":\"20McVa9n\",\"about_privacy.md\":\"CcWPOTgs\",\"about_releases.md\":\"GqqAVeGY\",\"about_team.md\":\"Bnw40y2b\",\"chatai_1.md\":\"HgVg4ZaF\",\"chatai_10.md\":\"CTnXLc7S\",\"chatai_11.md\":\"Bnu6XwVR\",\"chatai_12.md\":\"BjHlYlin\",\"chatai_13.md\":\"DJ-bKzOz\",\"chatai_14.md\":\"DoZ5hP4U\",\"chatai_15.md\":\"FqlGfoIe\",\"chatai_16.md\":\"CzoepajQ\",\"chatai_17.md\":\"BnjtHHP_\",\"chatai_18.md\":\"D8S5vx2v\",\"chatai_19.md\":\"BOb4ImDV\",\"chatai_2.md\":\"D7bC90AH\",\"chatai_20.md\":\"BI3SLXl_\",\"chatai_21.md\":\"DkyRvrUP\",\"chatai_22.md\":\"B237UNEP\",\"chatai_23.md\":\"D1VIvVYN\",\"chatai_24.md\":\"C8GKHVdy\",\"chatai_25.md\":\"BZlIKSa_\",\"chatai_26.md\":\"CLrDD-JH\",\"chatai_27.md\":\"C1mZAar3\",\"chatai_28.md\":\"TdO3WPxg\",\"chatai_29.md\":\"eWsI2Y0u\",\"chatai_3.md\":\"dbXQWKxB\",\"chatai_30.md\":\"BT7h6CNx\",\"chatai_31.md\":\"Bp3J6je_\",\"chatai_32.md\":\"Ca5HE3G1\",\"chatai_33.md\":\"CFiN68eh\",\"chatai_34.md\":\"fC17qNJI\",\"chatai_35.md\":\"BxkKjXHF\",\"chatai_36.md\":\"CUFGGjoy\",\"chatai_37.md\":\"BxFRpqBM\",\"chatai_38.md\":\"1u3oJt6u\",\"chatai_39.md\":\"DaWWhZ_E\",\"chatai_4.md\":\"CbCT1UY0\",\"chatai_40.md\":\"D5mGRd88\",\"chatai_41.md\":\"Cvk75GMt\",\"chatai_42.md\":\"CpVn7Ll8\",\"chatai_43.md\":\"BgvggX7t\",\"chatai_44.md\":\"PU0pmcKx\",\"chatai_45.md\":\"Ju-KQY07\",\"chatai_46.md\":\"C6eePUN3\",\"chatai_47.md\":\"BexhBinr\",\"chatai_48.md\":\"BzLi8UE1\",\"chatai_49.md\":\"C2XUHJF3\",\"chatai_5.md\":\"C4iZmTZn\",\"chatai_50.md\":\"CqRudc2K\",\"chatai_51.md\":\"BJmKWSrc\",\"chatai_52.md\":\"Dbzqs2dt\",\"chatai_53.md\":\"CdWIU-ib\",\"chatai_54.md\":\"BfFE9vXc\",\"chatai_55.md\":\"BFtaz2_k\",\"chatai_56.md\":\"BCz8AKyy\",\"chatai_57.md\":\"DEUVd1KO\",\"chatai_58.md\":\"C68ldjed\",\"chatai_59.md\":\"CoMd6Dtr\",\"chatai_6.md\":\"DPA4d22o\",\"chatai_60.md\":\"CpHun69H\",\"chatai_61.md\":\"Bj-ME1Ra\",\"chatai_62.md\":\"CxPPudwx\",\"chatai_63.md\":\"B7WjX9Vr\",\"chatai_64.md\":\"Clwb1sRd\",\"chatai_65.md\":\"DzK-sSqx\",\"chatai_66.md\":\"BxY0G9Ls\",\"chatai_67.md\":\"ufMWZujy\",\"chatai_68.md\":\"CTqCnfhP\",\"chatai_7.md\":\"9Az81a_b\",\"chatai_8.md\":\"DHsmoWtS\",\"chatai_9.md\":\"qimIVND6\",\"chatai_index.md\":\"hL1i_FIH\",\"deepseek_1.md\":\"CC0dyOFC\",\"deepseek_10.md\":\"D3Utsf8j\",\"deepseek_11.md\":\"DOnw1b4I\",\"deepseek_12.md\":\"DVt33RR9\",\"deepseek_13.md\":\"2zAHGCQm\",\"deepseek_14.md\":\"22NXfJAZ\",\"deepseek_15.md\":\"fNVLXz-J\",\"deepseek_16.md\":\"D3l7otGJ\",\"deepseek_17.md\":\"BFt9TSp0\",\"deepseek_18.md\":\"Cpu4G6MD\",\"deepseek_19.md\":\"Cbvw8mX6\",\"deepseek_2.md\":\"D6Q8kiyy\",\"deepseek_20.md\":\"BRDLr8Ge\",\"deepseek_21.md\":\"5gGJA47O\",\"deepseek_22.md\":\"C6ypFPlP\",\"deepseek_23.md\":\"DW0yN_YB\",\"deepseek_24.md\":\"LrgYnFl6\",\"deepseek_25.md\":\"CWdpG-sG\",\"deepseek_26.md\":\"Fzk2Zz8X\",\"deepseek_27.md\":\"DyadJsO7\",\"deepseek_28.md\":\"6xYBdsnT\",\"deepseek_29.md\":\"whxnyT2B\",\"deepseek_3.md\":\"Vq1rA8T1\",\"deepseek_30.md\":\"dBm8g9i6\",\"deepseek_31.md\":\"CK4dWOSm\",\"deepseek_32.md\":\"BDHONTdd\",\"deepseek_33.md\":\"BVYfNIER\",\"deepseek_34.md\":\"DdypipeO\",\"deepseek_35.md\":\"BvkLkBDn\",\"deepseek_36.md\":\"DRAjq0Ca\",\"deepseek_37.md\":\"CzGWSu86\",\"deepseek_38.md\":\"D9ZkGgT0\",\"deepseek_39.md\":\"Bp79W5Ca\",\"deepseek_4.md\":\"BWjJ6bzf\",\"deepseek_40.md\":\"wC5Tor8_\",\"deepseek_41.md\":\"g9bMGbQk\",\"deepseek_42.md\":\"CQN2B4c8\",\"deepseek_43.md\":\"D-urEp9B\",\"deepseek_44.md\":\"q-xEhf3v\",\"deepseek_45.md\":\"DzchJ6Rp\",\"deepseek_46.md\":\"BZ3uxas7\",\"deepseek_47.md\":\"Cz_NfdnB\",\"deepseek_48.md\":\"D5rY-6Ji\",\"deepseek_49.md\":\"Btg3BbLI\",\"deepseek_5.md\":\"b3rmH3X6\",\"deepseek_50.md\":\"DFgL4Tu3\",\"deepseek_51.md\":\"Bg_L4skk\",\"deepseek_52.md\":\"3eo7G6Jz\",\"deepseek_53.md\":\"B8-Lnj0X\",\"deepseek_54.md\":\"BVWxM8hN\",\"deepseek_55.md\":\"DytlGl-A\",\"deepseek_56.md\":\"CSWKCRJL\",\"deepseek_57.md\":\"BQqsbxhg\",\"deepseek_58.md\":\"DfrSD8nt\",\"deepseek_59.md\":\"DvCChJna\",\"deepseek_6.md\":\"Chelw_a7\",\"deepseek_60.md\":\"BboivcvG\",\"deepseek_61.md\":\"DKXmQDNd\",\"deepseek_62.md\":\"B0_9JQsr\",\"deepseek_63.md\":\"DRJaAU1l\",\"deepseek_64.md\":\"DqI5Vxke\",\"deepseek_65.md\":\"BbE_hXqV\",\"deepseek_66.md\":\"1dvxRFPL\",\"deepseek_67.md\":\"DE0MgIZO\",\"deepseek_68.md\":\"tsoV6l1L\",\"deepseek_7.md\":\"DVnjE48c\",\"deepseek_8.md\":\"sfVY2Mjz\",\"deepseek_9.md\":\"04Qsn_zZ\",\"drive_1.md\":\"Bw5Nbg9c\",\"drive_10.md\":\"CSCiwb8x\",\"drive_11.md\":\"vu4GxIeD\",\"drive_12.md\":\"Ciwgb0QZ\",\"drive_13.md\":\"DQtV9crl\",\"drive_14.md\":\"d19eEQne\",\"drive_15.md\":\"fTRLsIMr\",\"drive_16.md\":\"Cjlup0RK\",\"drive_17.md\":\"Ip1gbnnx\",\"drive_18.md\":\"HjecXKce\",\"drive_19.md\":\"D6v1tN_x\",\"drive_2.md\":\"B2jv0Nh3\",\"drive_20.md\":\"BevkjKfx\",\"drive_21.md\":\"DFREq1M1\",\"drive_22.md\":\"Q18u9OEN\",\"drive_23.md\":\"0Kkv3o2t\",\"drive_24.md\":\"C8l2Mhxh\",\"drive_25.md\":\"BcaazXnZ\",\"drive_26.md\":\"Bms5Ed2X\",\"drive_27.md\":\"XP3VyJqO\",\"drive_28.md\":\"BPO9ets4\",\"drive_29.md\":\"DP1J9APq\",\"drive_3.md\":\"DOdLkOgE\",\"drive_30.md\":\"oOvh1F1b\",\"drive_31.md\":\"Eykn0qdl\",\"drive_32.md\":\"C7Bta0CU\",\"drive_33.md\":\"QFZaA5PK\",\"drive_34.md\":\"CqtKqbQA\",\"drive_35.md\":\"E-hAy0Tz\",\"drive_36.md\":\"Buu8DDrs\",\"drive_37.md\":\"Dmx4hQDZ\",\"drive_38.md\":\"Bk1velMM\",\"drive_39.md\":\"N5xeSrOi\",\"drive_4.md\":\"XKYS0nL2\",\"drive_40.md\":\"B9y4TzkC\",\"drive_41.md\":\"2OiT-sBg\",\"drive_42.md\":\"B9OWFNWj\",\"drive_43.md\":\"DK40VPca\",\"drive_44.md\":\"8BcSNpvf\",\"drive_45.md\":\"B_4i9STk\",\"drive_46.md\":\"CwcNDFP2\",\"drive_47.md\":\"CRQCiuhp\",\"drive_48.md\":\"B0azy60E\",\"drive_49.md\":\"BkHrUIgz\",\"drive_5.md\":\"fAUar9md\",\"drive_50.md\":\"DjpaBLfR\",\"drive_51.md\":\"lynMps7W\",\"drive_52.md\":\"BF79U-sd\",\"drive_53.md\":\"DgUvy-j5\",\"drive_54.md\":\"CbKCeM9C\",\"drive_55.md\":\"Bv9AGjsS\",\"drive_56.md\":\"DKG-Dr9P\",\"drive_57.md\":\"apRYW3Nx\",\"drive_58.md\":\"NBUZ3b1p\",\"drive_59.md\":\"DytTh2h6\",\"drive_6.md\":\"CNdEdzkf\",\"drive_60.md\":\"CRDhaYfj\",\"drive_7.md\":\"BfwBDTqe\",\"drive_8.md\":\"DcrNyRT2\",\"drive_9.md\":\"BNJewnH8\",\"drive_donation.md\":\"8wvNJAVf\",\"drive_prompt.md\":\"DuFBIqs0\",\"drive_promptlibrary.md\":\"D2zvzxSB\",\"drive_team.md\":\"CkNmWMDG\",\"ecosystem_deepseek.md\":\"BU8mlKeX\",\"ecosystem_navigation.md\":\"wKlND_1J\",\"ecosystem_newsletters.md\":\"BhroXMnV\",\"ecosystem_themes.md\":\"DcxYtm4j\",\"error-reference_index.md\":\"C8cWCSv1\",\"examples_index.md\":\"DYIYBDwT\",\"grok_1.md\":\"DMTBoRHm\",\"grok_10.md\":\"DSTsEpHw\",\"grok_11.md\":\"BOuj_Gsl\",\"grok_12.md\":\"D4dXr3v-\",\"grok_13.md\":\"D6p-yLGr\",\"grok_14.md\":\"2Uq4YPeJ\",\"grok_15.md\":\"DJKuxe2B\",\"grok_16.md\":\"CQ3oSfeZ\",\"grok_17.md\":\"DhTP3F5w\",\"grok_18.md\":\"CbOFEmjN\",\"grok_19.md\":\"SG_eYOb9\",\"grok_2.md\":\"CxGRyG4k\",\"grok_20.md\":\"C3s8JXvL\",\"grok_21.md\":\"BD24YgKj\",\"grok_22.md\":\"CzxMQSfs\",\"grok_23.md\":\"DrPY8FCm\",\"grok_24.md\":\"CNDMAYzi\",\"grok_25.md\":\"kqyKv3YF\",\"grok_26.md\":\"RpJFpS5s\",\"grok_27.md\":\"D2uzb-qI\",\"grok_28.md\":\"Cn2VjyOJ\",\"grok_29.md\":\"Bfa4xtOZ\",\"grok_3.md\":\"z0y8Mo_p\",\"grok_30.md\":\"C2ZGLuWn\",\"grok_31.md\":\"CaS5GbY6\",\"grok_32.md\":\"DOm3yFuo\",\"grok_33.md\":\"D1mNgupx\",\"grok_34.md\":\"DV6DDyNI\",\"grok_35.md\":\"COCGcf_B\",\"grok_36.md\":\"DuZfMDSX\",\"grok_37.md\":\"B3SKL9CV\",\"grok_38.md\":\"8sdMXF2y\",\"grok_39.md\":\"Co2edb7x\",\"grok_4.md\":\"BkhjtO7W\",\"grok_40.md\":\"BL-bw-6D\",\"grok_41.md\":\"B5RHJuTZ\",\"grok_42.md\":\"DVUXwaP5\",\"grok_43.md\":\"5qNLlXto\",\"grok_44.md\":\"zdIjJwFd\",\"grok_45.md\":\"89mh_qeV\",\"grok_46.md\":\"B9lVwshf\",\"grok_47.md\":\"CZHNLmr-\",\"grok_48.md\":\"CBr8i5_i\",\"grok_49.md\":\"B6esobtK\",\"grok_5.md\":\"DyvmAtbX\",\"grok_50.md\":\"CXJ6U5MY\",\"grok_51.md\":\"CPDGm7rU\",\"grok_52.md\":\"CF-yQFb8\",\"grok_53.md\":\"QUk5QUN2\",\"grok_54.md\":\"B7l4v7rg\",\"grok_55.md\":\"DaqaazdL\",\"grok_56.md\":\"oYNi8qOS\",\"grok_57.md\":\"Diiy1POO\",\"grok_58.md\":\"rrbyr0sk\",\"grok_59.md\":\"-v-vjGL6\",\"grok_6.md\":\"CFWsiNQE\",\"grok_60.md\":\"CEGWVq9b\",\"grok_61.md\":\"BDhcszxg\",\"grok_62.md\":\"D3mB0xZv\",\"grok_63.md\":\"DU9W_QFJ\",\"grok_64.md\":\"DiKXif3R\",\"grok_65.md\":\"BoOfb5AO\",\"grok_66.md\":\"DMejfYjb\",\"grok_67.md\":\"SqSEd7e8\",\"grok_68.md\":\"DLFeOGHO\",\"grok_7.md\":\"D-QlBh91\",\"grok_8.md\":\"S0i6C_iZ\",\"grok_9.md\":\"BuxPVnfz\",\"guide_1.md\":\"Bxi8cEHj\",\"guide_10.md\":\"BY3r6NxC\",\"guide_11.md\":\"B9e_T69k\",\"guide_12.md\":\"q7SuRjZ1\",\"guide_13.md\":\"CFX-4tGg\",\"guide_14.md\":\"B8ggSs08\",\"guide_15.md\":\"DsujHAII\",\"guide_16.md\":\"CyYwOC3I\",\"guide_17.md\":\"BQ3aNk-N\",\"guide_18.md\":\"DFMfu2mf\",\"guide_19.md\":\"Dapmrqfs\",\"guide_2.md\":\"DhyR8Xe7\",\"guide_20.md\":\"D00G5qOn\",\"guide_21.md\":\"MCQFlybg\",\"guide_22.md\":\"Dz50Bmp9\",\"guide_23.md\":\"TDOXEu1f\",\"guide_24.md\":\"BQSBcHV5\",\"guide_25.md\":\"zT1RL3rP\",\"guide_26.md\":\"B6DgQsiQ\",\"guide_27.md\":\"C5g8T_8X\",\"guide_28.md\":\"BDtmAUOC\",\"guide_29.md\":\"DM_4z5PD\",\"guide_3.md\":\"U8Hec-bm\",\"guide_30.md\":\"7ZYoQ7FF\",\"guide_31.md\":\"B7eueUmf\",\"guide_32.md\":\"rV7bg97i\",\"guide_33.md\":\"0GwkwhSb\",\"guide_34.md\":\"BCP0iaf9\",\"guide_35.md\":\"BXSOsA_b\",\"guide_36.md\":\"zM8Da746\",\"guide_37.md\":\"CQUwSFrH\",\"guide_38.md\":\"DwXR2zsF\",\"guide_39.md\":\"CZexsWZU\",\"guide_4.md\":\"Owud_30u\",\"guide_40.md\":\"BKnbWnIf\",\"guide_41.md\":\"Sto94BB-\",\"guide_42.md\":\"B-r8tTf2\",\"guide_43.md\":\"Bceeb6S1\",\"guide_44.md\":\"DrgEqFBA\",\"guide_45.md\":\"C6cPc-wO\",\"guide_46.md\":\"D3-Gpqqp\",\"guide_47.md\":\"BWstU8bt\",\"guide_48.md\":\"fLy6-_aI\",\"guide_49.md\":\"C60uomQ4\",\"guide_5.md\":\"B4Hzbx7x\",\"guide_50.md\":\"CSXDxglx\",\"guide_51.md\":\"DdJ7Zxnw\",\"guide_52.md\":\"DANBvDtP\",\"guide_53.md\":\"BZ2mRZaq\",\"guide_54.md\":\"DjitNciB\",\"guide_55.md\":\"Dtg4Nj20\",\"guide_56.md\":\"DUxhDIyg\",\"guide_57.md\":\"Dl4zC68J\",\"guide_58.md\":\"Dr3OIFmP\",\"guide_59.md\":\"C8s66Vhe\",\"guide_6.md\":\"9_4BXrZT\",\"guide_60.md\":\"CJASHr3O\",\"guide_61.md\":\"B9pXTX-Z\",\"guide_62.md\":\"DWY5lc7P\",\"guide_63.md\":\"hTTE1X5e\",\"guide_64.md\":\"ByJxO3Vh\",\"guide_65.md\":\"7RMX3_kN\",\"guide_66.md\":\"C9eTAGNG\",\"guide_67.md\":\"DLkgGLu4\",\"guide_68.md\":\"CLDieSP-\",\"guide_7.md\":\"Dzs-BiaG\",\"guide_8.md\":\"Cgh6eAxq\",\"guide_9.md\":\"DpTW3FfV\",\"index.md\":\"Bp1nLQk1\",\"library_1.md\":\"C7RnRKWC\",\"library_10.md\":\"Bc8wZpKC\",\"library_11.md\":\"DJHlfFa1\",\"library_12.md\":\"D3MX1rbv\",\"library_13.md\":\"ZtnwBWBq\",\"library_14.md\":\"qWQ1xsAX\",\"library_15.md\":\"I0PG01F3\",\"library_16.md\":\"BSJojk5O\",\"library_17.md\":\"BYbQUEPV\",\"library_18.md\":\"ZB8XzbuW\",\"library_19.md\":\"NMQqvpwv\",\"library_2.md\":\"DZfbk4ko\",\"library_20.md\":\"DiIgf-3N\",\"library_21.md\":\"CTiHxf_b\",\"library_22.md\":\"DmyMYM4_\",\"library_23.md\":\"Dxtrztyt\",\"library_24.md\":\"p6Ar5W_O\",\"library_25.md\":\"Cmtaxz9K\",\"library_26.md\":\"yQVlt8RN\",\"library_27.md\":\"BkucTdRM\",\"library_28.md\":\"BmvCvdvn\",\"library_29.md\":\"DhkDlbMX\",\"library_3.md\":\"BOpov-Li\",\"library_30.md\":\"B4XlDtql\",\"library_31.md\":\"Dzoe88no\",\"library_32.md\":\"28qGyEK1\",\"library_33.md\":\"MFkg4v6t\",\"library_34.md\":\"CQzb1lNt\",\"library_35.md\":\"fP4URcpe\",\"library_36.md\":\"DtviE1EY\",\"library_37.md\":\"BdE22j8i\",\"library_38.md\":\"BURm33ly\",\"library_39.md\":\"C_T6paCO\",\"library_4.md\":\"SBhglBm3\",\"library_40.md\":\"CY97ukVf\",\"library_41.md\":\"BFY9GN9e\",\"library_42.md\":\"DRxrJwDB\",\"library_43.md\":\"CqQXOcBV\",\"library_44.md\":\"WRUKFVFx\",\"library_45.md\":\"CV4RLDeW\",\"library_46.md\":\"erRDQqZ_\",\"library_47.md\":\"7q0nj4Yj\",\"library_48.md\":\"D2oYyYhI\",\"library_49.md\":\"DOU0w75c\",\"library_5.md\":\"Bq5IB0pK\",\"library_50.md\":\"DRvNMbLZ\",\"library_51.md\":\"DV8CRhTe\",\"library_52.md\":\"DgFCLQ4A\",\"library_53.md\":\"Y_X8Sz1W\",\"library_54.md\":\"Crw-CPFj\",\"library_55.md\":\"CJj8Xprk\",\"library_56.md\":\"6VwFXy8m\",\"library_57.md\":\"CLYhtOi3\",\"library_58.md\":\"IGru-59k\",\"library_59.md\":\"BjzJ6GVb\",\"library_6.md\":\"D312LgMw\",\"library_60.md\":\"CzjwAxFm\",\"library_61.md\":\"Cl9n_r4s\",\"library_62.md\":\"C4k5gPZK\",\"library_63.md\":\"Big7B45a\",\"library_64.md\":\"BwufWI6n\",\"library_65.md\":\"CZ1mY6zZ\",\"library_66.md\":\"D8ow5HIJ\",\"library_67.md\":\"DdwcBLg3\",\"library_68.md\":\"CVQWr-R2\",\"library_7.md\":\"RoUXi3rp\",\"library_8.md\":\"C1yQVxOy\",\"library_9.md\":\"ChB8uEpi\",\"partners_all.md\":\"DrI_oKr-\",\"partners_curotec.md\":\"C5GS61rR\",\"partners_herodevs.md\":\"BVDxGK-m\",\"partners_index.md\":\"BBcUVcAI\",\"partners_monterail.md\":\"B627I69r\",\"partners_passionatepeople.md\":\"BDAHuxir\",\"partners_redberry.md\":\"B-Sf3MRF\",\"partners_vehikl.md\":\"CjJYaD5l\",\"partners_webreinvent.md\":\"CuHjowhF\",\"quotes_1.md\":\"Cq3dVw1V\",\"quotes_10.md\":\"Lv4aIcP_\",\"quotes_11.md\":\"DuW5TJS0\",\"quotes_12.md\":\"BjCA30F4\",\"quotes_13.md\":\"C13WG3Ft\",\"quotes_14.md\":\"Ft7GYQLB\",\"quotes_15.md\":\"DW6KVhoF\",\"quotes_16.md\":\"CKYX5yHE\",\"quotes_17.md\":\"vPJaeh-B\",\"quotes_18.md\":\"DeXHg5RD\",\"quotes_19.md\":\"J95HuL4s\",\"quotes_2.md\":\"CSTYaKBs\",\"quotes_20.md\":\"CTT29L95\",\"quotes_21.md\":\"BSPDqmZq\",\"quotes_22.md\":\"BY5YXyUh\",\"quotes_23.md\":\"C6mNt647\",\"quotes_24.md\":\"BfUOq9va\",\"quotes_25.md\":\"yqZuKxoW\",\"quotes_26.md\":\"ChT6MEZM\",\"quotes_27.md\":\"B2x2ac3k\",\"quotes_28.md\":\"CConlTQv\",\"quotes_29.md\":\"BbV1Qv9Q\",\"quotes_3.md\":\"DOUJ6skd\",\"quotes_30.md\":\"D6vFUrbt\",\"quotes_31.md\":\"CgY-kXwA\",\"quotes_32.md\":\"C0v0Erpn\",\"quotes_33.md\":\"CFF5vQfw\",\"quotes_34.md\":\"sSp1f6M_\",\"quotes_35.md\":\"rJy7YWaS\",\"quotes_36.md\":\"Bbtc6mQ1\",\"quotes_37.md\":\"BS4f8hm8\",\"quotes_38.md\":\"DeI79yDi\",\"quotes_39.md\":\"DaoWOXex\",\"quotes_4.md\":\"lqPg8WTp\",\"quotes_40.md\":\"Cnd_3uyt\",\"quotes_41.md\":\"OF4zxtMt\",\"quotes_42.md\":\"Dem42MCQ\",\"quotes_43.md\":\"C1obI3Cv\",\"quotes_44.md\":\"CKkCFVfW\",\"quotes_45.md\":\"D7kAEtnM\",\"quotes_46.md\":\"DeP_wfzm\",\"quotes_47.md\":\"DcHnKYtP\",\"quotes_48.md\":\"Cu6GUZr6\",\"quotes_49.md\":\"Bzm3DWiv\",\"quotes_5.md\":\"Cax-_Yo9\",\"quotes_50.md\":\"BG-rpW4D\",\"quotes_51.md\":\"D4Dq9cxs\",\"quotes_52.md\":\"XE-NChy6\",\"quotes_53.md\":\"XYfEPL1T\",\"quotes_54.md\":\"-oUTMyA4\",\"quotes_55.md\":\"CzAoAXVD\",\"quotes_56.md\":\"Cf46R68x\",\"quotes_57.md\":\"CcAkrGL_\",\"quotes_58.md\":\"BW-iyR2F\",\"quotes_59.md\":\"CBcznBT3\",\"quotes_6.md\":\"DRwNUjho\",\"quotes_60.md\":\"DOaKxrh0\",\"quotes_61.md\":\"D8d47W-o\",\"quotes_62.md\":\"De4LZLAe\",\"quotes_63.md\":\"ItEHtl1O\",\"quotes_64.md\":\"DvgIcw-v\",\"quotes_65.md\":\"CY7S0Qga\",\"quotes_66.md\":\"XlxFT6ZA\",\"quotes_67.md\":\"QLmKikbR\",\"quotes_68.md\":\"B8WPR8DV\",\"quotes_7.md\":\"BYURwBlu\",\"quotes_8.md\":\"4ep4_ZnG\",\"quotes_9.md\":\"DLcyV6-Z\",\"swap_app.md\":\"dX62wfc0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh\",\"dir\":\"ltr\",\"title\":\"MIT\",\"description\":\"MIT\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo.svg\",\"nav\":[{\"text\":\"data\",\"activeMatch\":\"^/drive/\",\"items\":[{\"text\":\"data1\",\"link\":\"/drive/1\"},{\"text\":\"data2\",\"link\":\"/drive/2\"},{\"text\":\"data3\",\"link\":\"/drive/3\"},{\"text\":\"data4\",\"link\":\"/drive/4\"},{\"text\":\"data5\",\"link\":\"/drive/5\"},{\"text\":\"data6\",\"link\":\"/drive/6\"},{\"text\":\"data7\",\"link\":\"/drive/7\"},{\"text\":\"data8\",\"link\":\"/drive/8\"},{\"text\":\"data9\",\"link\":\"/drive/9\"},{\"text\":\"data10\",\"link\":\"/drive/10\"},{\"text\":\"data11\",\"link\":\"/drive/11\"},{\"text\":\"data12\",\"link\":\"/drive/12\"},{\"text\":\"data13\",\"link\":\"/drive/13\"},{\"text\":\"data14\",\"link\":\"/drive/14\"},{\"text\":\"data15\",\"link\":\"/drive/15\"},{\"text\":\"data16\",\"link\":\"/drive/16\"},{\"text\":\"data17\",\"link\":\"/drive/17\"},{\"text\":\"data18\",\"link\":\"/drive/18\"},{\"text\":\"data19\",\"link\":\"/drive/19\"},{\"text\":\"data20\",\"link\":\"/drive/20\"},{\"text\":\"data21\",\"link\":\"/drive/21\"},{\"text\":\"data22\",\"link\":\"/drive/22\"},{\"text\":\"data23\",\"link\":\"/drive/23\"},{\"text\":\"data24\",\"link\":\"/drive/24\"},{\"text\":\"data25\",\"link\":\"/drive/25\"},{\"text\":\"data26\",\"link\":\"/drive/26\"},{\"text\":\"data27\",\"link\":\"/drive/27\"},{\"text\":\"data28\",\"link\":\"/drive/28\"},{\"text\":\"data29\",\"link\":\"/drive/29\"},{\"text\":\"data30\",\"link\":\"/drive/30\"},{\"text\":\"data31\",\"link\":\"/drive/31\"},{\"text\":\"data32\",\"link\":\"/drive/32\"},{\"text\":\"data33\",\"link\":\"/drive/33\"},{\"text\":\"data34\",\"link\":\"/drive/34\"},{\"text\":\"data35\",\"link\":\"/drive/35\"},{\"text\":\"data36\",\"link\":\"/drive/36\"},{\"text\":\"data37\",\"link\":\"/drive/37\"},{\"text\":\"data38\",\"link\":\"/drive/38\"},{\"text\":\"data39\",\"link\":\"/drive/39\"},{\"text\":\"data40\",\"link\":\"/drive/40\"},{\"text\":\"data41\",\"link\":\"/drive/41\"},{\"text\":\"data42\",\"link\":\"/drive/42\"},{\"text\":\"data43\",\"link\":\"/drive/43\"},{\"text\":\"data44\",\"link\":\"/drive/44\"},{\"text\":\"data45\",\"link\":\"/drive/45\"},{\"text\":\"data46\",\"link\":\"/drive/46\"},{\"text\":\"data47\",\"link\":\"/drive/47\"},{\"text\":\"data48\",\"link\":\"/drive/48\"},{\"text\":\"data49\",\"link\":\"/drive/49\"},{\"text\":\"data50\",\"link\":\"/drive/50\"},{\"text\":\"data51\",\"link\":\"/drive/51\"},{\"text\":\"data52\",\"link\":\"/drive/52\"},{\"text\":\"data53\",\"link\":\"/drive/53\"},{\"text\":\"data54\",\"link\":\"/drive/54\"},{\"text\":\"data55\",\"link\":\"/drive/55\"},{\"text\":\"data56\",\"link\":\"/drive/56\"},{\"text\":\"data57\",\"link\":\"/drive/57\"},{\"text\":\"data58\",\"link\":\"/drive/58\"},{\"text\":\"data59\",\"link\":\"/drive/59\"},{\"text\":\"data60\",\"link\":\"/drive/60\"}]},{\"text\":\"grok\",\"activeMatch\":\"^/grok/\",\"items\":[{\"text\":\"grok1\",\"link\":\"/grok/1\"},{\"text\":\"grok2\",\"link\":\"/grok/2\"},{\"text\":\"grok3\",\"link\":\"/grok/3\"},{\"text\":\"grok4\",\"link\":\"/grok/4\"},{\"text\":\"grok5\",\"link\":\"/grok/5\"},{\"text\":\"grok6\",\"link\":\"/grok/6\"},{\"text\":\"grok7\",\"link\":\"/grok/7\"},{\"text\":\"grok8\",\"link\":\"/grok/8\"},{\"text\":\"grok9\",\"link\":\"/grok/9\"},{\"text\":\"grok10\",\"link\":\"/grok/10\"},{\"text\":\"grok11\",\"link\":\"/grok/11\"},{\"text\":\"grok12\",\"link\":\"/grok/12\"},{\"text\":\"grok13\",\"link\":\"/grok/13\"},{\"text\":\"grok14\",\"link\":\"/grok/14\"},{\"text\":\"grok15\",\"link\":\"/grok/15\"},{\"text\":\"grok16\",\"link\":\"/grok/16\"},{\"text\":\"grok17\",\"link\":\"/grok/17\"},{\"text\":\"grok18\",\"link\":\"/grok/18\"},{\"text\":\"grok19\",\"link\":\"/grok/19\"},{\"text\":\"grok20\",\"link\":\"/grok/20\"},{\"text\":\"grok21\",\"link\":\"/grok/21\"},{\"text\":\"grok22\",\"link\":\"/grok/22\"},{\"text\":\"grok23\",\"link\":\"/grok/23\"},{\"text\":\"grok24\",\"link\":\"/grok/24\"},{\"text\":\"grok25\",\"link\":\"/grok/25\"},{\"text\":\"grok26\",\"link\":\"/grok/26\"},{\"text\":\"grok27\",\"link\":\"/grok/27\"},{\"text\":\"grok28\",\"link\":\"/grok/28\"},{\"text\":\"grok29\",\"link\":\"/grok/29\"},{\"text\":\"grok30\",\"link\":\"/grok/30\"},{\"text\":\"grok31\",\"link\":\"/grok/31\"},{\"text\":\"grok32\",\"link\":\"/grok/32\"},{\"text\":\"grok33\",\"link\":\"/grok/33\"},{\"text\":\"grok34\",\"link\":\"/grok/34\"},{\"text\":\"grok35\",\"link\":\"/grok/35\"},{\"text\":\"grok36\",\"link\":\"/grok/36\"},{\"text\":\"grok37\",\"link\":\"/grok/37\"},{\"text\":\"grok38\",\"link\":\"/grok/38\"},{\"text\":\"grok39\",\"link\":\"/grok/39\"},{\"text\":\"grok40\",\"link\":\"/grok/40\"},{\"text\":\"grok41\",\"link\":\"/grok/41\"},{\"text\":\"grok42\",\"link\":\"/grok/42\"},{\"text\":\"grok43\",\"link\":\"/grok/43\"},{\"text\":\"grok44\",\"link\":\"/grok/44\"},{\"text\":\"grok45\",\"link\":\"/grok/45\"},{\"text\":\"grok46\",\"link\":\"/grok/46\"},{\"text\":\"grok47\",\"link\":\"/grok/47\"},{\"text\":\"grok48\",\"link\":\"/grok/48\"},{\"text\":\"grok49\",\"link\":\"/grok/49\"},{\"text\":\"grok50\",\"link\":\"/grok/50\"},{\"text\":\"grok51\",\"link\":\"/grok/51\"},{\"text\":\"grok52\",\"link\":\"/grok/52\"},{\"text\":\"grok53\",\"link\":\"/grok/53\"},{\"text\":\"grok54\",\"link\":\"/grok/54\"},{\"text\":\"grok55\",\"link\":\"/grok/55\"},{\"text\":\"grok56\",\"link\":\"/grok/56\"},{\"text\":\"grok57\",\"link\":\"/grok/57\"},{\"text\":\"grok58\",\"link\":\"/grok/58\"},{\"text\":\"grok59\",\"link\":\"/grok/59\"},{\"text\":\"grok60\",\"link\":\"/grok/60\"},{\"text\":\"grok61\",\"link\":\"/grok/61\"},{\"text\":\"grok62\",\"link\":\"/grok/62\"},{\"text\":\"grok63\",\"link\":\"/grok/63\"},{\"text\":\"grok64\",\"link\":\"/grok/64\"},{\"text\":\"grok65\",\"link\":\"/grok/65\"},{\"text\":\"grok66\",\"link\":\"/grok/66\"},{\"text\":\"grok67\",\"link\":\"/grok/67\"},{\"text\":\"grok68\",\"link\":\"/grok/68\"}]},{\"text\":\"wiki\",\"activeMatch\":\"^/guide/\",\"items\":[{\"text\":\"wiki1\",\"link\":\"/guide/1\"},{\"text\":\"wiki2\",\"link\":\"/guide/2\"},{\"text\":\"wiki3\",\"link\":\"/guide/3\"},{\"text\":\"wiki4\",\"link\":\"/guide/4\"},{\"text\":\"wiki5\",\"link\":\"/guide/5\"},{\"text\":\"wiki6\",\"link\":\"/guide/6\"},{\"text\":\"wiki7\",\"link\":\"/guide/7\"},{\"text\":\"wiki8\",\"link\":\"/guide/8\"},{\"text\":\"wiki9\",\"link\":\"/guide/9\"},{\"text\":\"wiki10\",\"link\":\"/guide/10\"},{\"text\":\"wiki11\",\"link\":\"/guide/11\"},{\"text\":\"wiki12\",\"link\":\"/guide/12\"},{\"text\":\"wiki13\",\"link\":\"/guide/13\"},{\"text\":\"wiki14\",\"link\":\"/guide/14\"},{\"text\":\"wiki15\",\"link\":\"/guide/15\"},{\"text\":\"wiki16\",\"link\":\"/guide/16\"},{\"text\":\"wiki17\",\"link\":\"/guide/17\"},{\"text\":\"wiki18\",\"link\":\"/guide/18\"},{\"text\":\"wiki19\",\"link\":\"/guide/19\"},{\"text\":\"wiki20\",\"link\":\"/guide/20\"},{\"text\":\"wiki21\",\"link\":\"/guide/21\"},{\"text\":\"wiki22\",\"link\":\"/guide/22\"},{\"text\":\"wiki23\",\"link\":\"/guide/23\"},{\"text\":\"wiki24\",\"link\":\"/guide/24\"},{\"text\":\"wiki25\",\"link\":\"/guide/25\"},{\"text\":\"wiki26\",\"link\":\"/guide/26\"},{\"text\":\"wiki27\",\"link\":\"/guide/27\"},{\"text\":\"wiki28\",\"link\":\"/guide/28\"},{\"text\":\"wiki29\",\"link\":\"/guide/29\"},{\"text\":\"wiki30\",\"link\":\"/guide/30\"},{\"text\":\"wiki31\",\"link\":\"/guide/31\"},{\"text\":\"wiki32\",\"link\":\"/guide/32\"},{\"text\":\"wiki33\",\"link\":\"/guide/33\"},{\"text\":\"wiki34\",\"link\":\"/guide/34\"},{\"text\":\"wiki35\",\"link\":\"/guide/35\"},{\"text\":\"wiki36\",\"link\":\"/guide/36\"},{\"text\":\"wiki37\",\"link\":\"/guide/37\"},{\"text\":\"wiki38\",\"link\":\"/guide/38\"},{\"text\":\"wiki39\",\"link\":\"/guide/39\"},{\"text\":\"wiki40\",\"link\":\"/guide/40\"},{\"text\":\"wiki41\",\"link\":\"/guide/41\"},{\"text\":\"wiki42\",\"link\":\"/guide/42\"},{\"text\":\"wiki43\",\"link\":\"/guide/43\"},{\"text\":\"wiki44\",\"link\":\"/guide/44\"},{\"text\":\"wiki45\",\"link\":\"/guide/45\"},{\"text\":\"wiki46\",\"link\":\"/guide/46\"},{\"text\":\"wiki47\",\"link\":\"/guide/47\"},{\"text\":\"wiki48\",\"link\":\"/guide/48\"},{\"text\":\"wiki49\",\"link\":\"/guide/49\"},{\"text\":\"wiki50\",\"link\":\"/guide/50\"},{\"text\":\"wiki51\",\"link\":\"/guide/51\"},{\"text\":\"wiki52\",\"link\":\"/guide/52\"},{\"text\":\"wiki53\",\"link\":\"/guide/53\"},{\"text\":\"wiki54\",\"link\":\"/guide/54\"},{\"text\":\"wiki55\",\"link\":\"/guide/55\"},{\"text\":\"wiki56\",\"link\":\"/guide/56\"},{\"text\":\"wiki57\",\"link\":\"/guide/57\"},{\"text\":\"wiki58\",\"link\":\"/guide/58\"},{\"text\":\"wiki59\",\"link\":\"/guide/59\"},{\"text\":\"wiki60\",\"link\":\"/guide/60\"},{\"text\":\"wiki61\",\"link\":\"/guide/61\"},{\"text\":\"wiki62\",\"link\":\"/guide/62\"},{\"text\":\"wiki63\",\"link\":\"/guide/63\"},{\"text\":\"wiki64\",\"link\":\"/guide/64\"},{\"text\":\"wiki65\",\"link\":\"/guide/65\"},{\"text\":\"wiki66\",\"link\":\"/guide/66\"},{\"text\":\"wiki67\",\"link\":\"/guide/67\"},{\"text\":\"wiki68\",\"link\":\"/guide/68\"}]},{\"text\":\"deep\",\"activeMatch\":\"^/deepseek/\",\"items\":[{\"text\":\"deep1\",\"link\":\"/deepseek/1\"},{\"text\":\"deep2\",\"link\":\"/deepseek/2\"},{\"text\":\"deep3\",\"link\":\"/deepseek/3\"},{\"text\":\"deep4\",\"link\":\"/deepseek/4\"},{\"text\":\"deep5\",\"link\":\"/deepseek/5\"},{\"text\":\"deep6\",\"link\":\"/deepseek/6\"},{\"text\":\"deep7\",\"link\":\"/deepseek/7\"},{\"text\":\"deep8\",\"link\":\"/deepseek/8\"},{\"text\":\"deep9\",\"link\":\"/deepseek/9\"},{\"text\":\"deep10\",\"link\":\"/deepseek/10\"},{\"text\":\"deep11\",\"link\":\"/deepseek/11\"},{\"text\":\"deep12\",\"link\":\"/deepseek/12\"},{\"text\":\"deep13\",\"link\":\"/deepseek/13\"},{\"text\":\"deep14\",\"link\":\"/deepseek/14\"},{\"text\":\"deep15\",\"link\":\"/deepseek/15\"},{\"text\":\"deep16\",\"link\":\"/deepseek/16\"},{\"text\":\"deep17\",\"link\":\"/deepseek/17\"},{\"text\":\"deep18\",\"link\":\"/deepseek/18\"},{\"text\":\"deep19\",\"link\":\"/deepseek/19\"},{\"text\":\"deep20\",\"link\":\"/deepseek/20\"},{\"text\":\"deep21\",\"link\":\"/deepseek/21\"},{\"text\":\"deep22\",\"link\":\"/deepseek/22\"},{\"text\":\"deep23\",\"link\":\"/deepseek/23\"},{\"text\":\"deep24\",\"link\":\"/deepseek/24\"},{\"text\":\"deep25\",\"link\":\"/deepseek/25\"},{\"text\":\"deep26\",\"link\":\"/deepseek/26\"},{\"text\":\"deep27\",\"link\":\"/deepseek/27\"},{\"text\":\"deep28\",\"link\":\"/deepseek/28\"},{\"text\":\"deep29\",\"link\":\"/deepseek/29\"},{\"text\":\"deep30\",\"link\":\"/deepseek/30\"},{\"text\":\"deep31\",\"link\":\"/deepseek/31\"},{\"text\":\"deep32\",\"link\":\"/deepseek/32\"},{\"text\":\"deep33\",\"link\":\"/deepseek/33\"},{\"text\":\"deep34\",\"link\":\"/deepseek/34\"},{\"text\":\"deep35\",\"link\":\"/deepseek/35\"},{\"text\":\"deep36\",\"link\":\"/deepseek/36\"},{\"text\":\"deep37\",\"link\":\"/deepseek/37\"},{\"text\":\"deep38\",\"link\":\"/deepseek/38\"},{\"text\":\"deep39\",\"link\":\"/deepseek/39\"},{\"text\":\"deep40\",\"link\":\"/deepseek/40\"},{\"text\":\"deep41\",\"link\":\"/deepseek/41\"},{\"text\":\"deep42\",\"link\":\"/deepseek/42\"},{\"text\":\"deep43\",\"link\":\"/deepseek/43\"},{\"text\":\"deep44\",\"link\":\"/deepseek/44\"},{\"text\":\"deep45\",\"link\":\"/deepseek/45\"},{\"text\":\"deep46\",\"link\":\"/deepseek/46\"},{\"text\":\"deep47\",\"link\":\"/deepseek/47\"},{\"text\":\"deep48\",\"link\":\"/deepseek/48\"},{\"text\":\"deep49\",\"link\":\"/deepseek/49\"},{\"text\":\"deep50\",\"link\":\"/deepseek/50\"},{\"text\":\"deep51\",\"link\":\"/deepseek/51\"},{\"text\":\"deep52\",\"link\":\"/deepseek/52\"},{\"text\":\"deep53\",\"link\":\"/deepseek/53\"},{\"text\":\"deep54\",\"link\":\"/deepseek/54\"},{\"text\":\"deep55\",\"link\":\"/deepseek/55\"},{\"text\":\"deep56\",\"link\":\"/deepseek/56\"},{\"text\":\"deep57\",\"link\":\"/deepseek/57\"},{\"text\":\"deep58\",\"link\":\"/deepseek/58\"},{\"text\":\"deep59\",\"link\":\"/deepseek/59\"},{\"text\":\"deep60\",\"link\":\"/deepseek/60\"},{\"text\":\"deep61\",\"link\":\"/deepseek/61\"},{\"text\":\"deep62\",\"link\":\"/deepseek/62\"},{\"text\":\"deep63\",\"link\":\"/deepseek/63\"},{\"text\":\"deep64\",\"link\":\"/deepseek/64\"},{\"text\":\"deep65\",\"link\":\"/deepseek/65\"},{\"text\":\"deep66\",\"link\":\"/deepseek/66\"},{\"text\":\"deep67\",\"link\":\"/deepseek/67\"},{\"text\":\"deep68\",\"link\":\"/deepseek/68\"}]},{\"text\":\"quotes\",\"activeMatch\":\"^/quotes/\",\"items\":[{\"text\":\"quotes1\",\"link\":\"/quotes/1\"},{\"text\":\"quotes2\",\"link\":\"/quotes/2\"},{\"text\":\"quotes3\",\"link\":\"/quotes/3\"},{\"text\":\"quotes4\",\"link\":\"/quotes/4\"},{\"text\":\"quotes5\",\"link\":\"/quotes/5\"},{\"text\":\"quotes6\",\"link\":\"/quotes/6\"},{\"text\":\"quotes7\",\"link\":\"/quotes/7\"},{\"text\":\"quotes8\",\"link\":\"/quotes/8\"},{\"text\":\"quotes9\",\"link\":\"/quotes/9\"},{\"text\":\"quotes10\",\"link\":\"/quotes/10\"},{\"text\":\"quotes11\",\"link\":\"/quotes/11\"},{\"text\":\"quotes12\",\"link\":\"/quotes/12\"},{\"text\":\"quotes13\",\"link\":\"/quotes/13\"},{\"text\":\"quotes14\",\"link\":\"/quotes/14\"},{\"text\":\"quotes15\",\"link\":\"/quotes/15\"},{\"text\":\"quotes16\",\"link\":\"/quotes/16\"},{\"text\":\"quotes17\",\"link\":\"/quotes/17\"},{\"text\":\"quotes18\",\"link\":\"/quotes/18\"},{\"text\":\"quotes19\",\"link\":\"/quotes/19\"},{\"text\":\"quotes20\",\"link\":\"/quotes/20\"},{\"text\":\"quotes21\",\"link\":\"/quotes/21\"},{\"text\":\"quotes22\",\"link\":\"/quotes/22\"},{\"text\":\"quotes23\",\"link\":\"/quotes/23\"},{\"text\":\"quotes24\",\"link\":\"/quotes/24\"},{\"text\":\"quotes25\",\"link\":\"/quotes/25\"},{\"text\":\"quotes26\",\"link\":\"/quotes/26\"},{\"text\":\"quotes27\",\"link\":\"/quotes/27\"},{\"text\":\"quotes28\",\"link\":\"/quotes/28\"},{\"text\":\"quotes29\",\"link\":\"/quotes/29\"},{\"text\":\"quotes30\",\"link\":\"/quotes/30\"},{\"text\":\"quotes31\",\"link\":\"/quotes/31\"},{\"text\":\"quotes32\",\"link\":\"/quotes/32\"},{\"text\":\"quotes33\",\"link\":\"/quotes/33\"},{\"text\":\"quotes34\",\"link\":\"/quotes/34\"},{\"text\":\"quotes35\",\"link\":\"/quotes/35\"},{\"text\":\"quotes36\",\"link\":\"/quotes/36\"},{\"text\":\"quotes37\",\"link\":\"/quotes/37\"},{\"text\":\"quotes38\",\"link\":\"/quotes/38\"},{\"text\":\"quotes39\",\"link\":\"/quotes/39\"},{\"text\":\"quotes40\",\"link\":\"/quotes/40\"},{\"text\":\"quotes41\",\"link\":\"/quotes/41\"},{\"text\":\"quotes42\",\"link\":\"/quotes/42\"},{\"text\":\"quotes43\",\"link\":\"/quotes/43\"},{\"text\":\"quotes44\",\"link\":\"/quotes/44\"},{\"text\":\"quotes45\",\"link\":\"/quotes/45\"},{\"text\":\"quotes46\",\"link\":\"/quotes/46\"},{\"text\":\"quotes47\",\"link\":\"/quotes/47\"},{\"text\":\"quotes48\",\"link\":\"/quotes/48\"},{\"text\":\"quotes49\",\"link\":\"/quotes/49\"},{\"text\":\"quotes50\",\"link\":\"/quotes/50\"},{\"text\":\"quotes51\",\"link\":\"/quotes/51\"},{\"text\":\"quotes52\",\"link\":\"/quotes/52\"},{\"text\":\"quotes53\",\"link\":\"/quotes/53\"},{\"text\":\"quotes54\",\"link\":\"/quotes/54\"},{\"text\":\"quotes55\",\"link\":\"/quotes/55\"},{\"text\":\"quotes56\",\"link\":\"/quotes/56\"},{\"text\":\"quotes57\",\"link\":\"/quotes/57\"},{\"text\":\"quotes58\",\"link\":\"/quotes/58\"},{\"text\":\"quotes59\",\"link\":\"/quotes/59\"},{\"text\":\"quotes60\",\"link\":\"/quotes/60\"},{\"text\":\"quotes61\",\"link\":\"/quotes/61\"},{\"text\":\"quotes62\",\"link\":\"/quotes/62\"},{\"text\":\"quotes63\",\"link\":\"/quotes/63\"},{\"text\":\"quotes64\",\"link\":\"/quotes/64\"},{\"text\":\"quotes65\",\"link\":\"/quotes/65\"},{\"text\":\"quotes66\",\"link\":\"/quotes/66\"},{\"text\":\"quotes67\",\"link\":\"/quotes/67\"},{\"text\":\"quotes68\",\"link\":\"/quotes/68\"}]},{\"text\":\"chatai\",\"activeMatch\":\"^/chatai/\",\"items\":[{\"text\":\"chatai1\",\"link\":\"/chatai/1\"},{\"text\":\"chatai2\",\"link\":\"/chatai/2\"},{\"text\":\"chatai3\",\"link\":\"/chatai/3\"},{\"text\":\"chatai4\",\"link\":\"/chatai/4\"},{\"text\":\"chatai5\",\"link\":\"/chatai/5\"},{\"text\":\"chatai6\",\"link\":\"/chatai/6\"},{\"text\":\"chatai7\",\"link\":\"/chatai/7\"},{\"text\":\"chatai8\",\"link\":\"/chatai/8\"},{\"text\":\"chatai9\",\"link\":\"/chatai/9\"},{\"text\":\"chatai10\",\"link\":\"/chatai/10\"},{\"text\":\"chatai11\",\"link\":\"/chatai/11\"},{\"text\":\"chatai12\",\"link\":\"/chatai/12\"},{\"text\":\"chatai13\",\"link\":\"/chatai/13\"},{\"text\":\"chatai14\",\"link\":\"/chatai/14\"},{\"text\":\"chatai15\",\"link\":\"/chatai/15\"},{\"text\":\"chatai16\",\"link\":\"/chatai/16\"},{\"text\":\"chatai17\",\"link\":\"/chatai/17\"},{\"text\":\"chatai18\",\"link\":\"/chatai/18\"},{\"text\":\"chatai19\",\"link\":\"/chatai/19\"},{\"text\":\"chatai20\",\"link\":\"/chatai/20\"},{\"text\":\"chatai21\",\"link\":\"/chatai/21\"},{\"text\":\"chatai22\",\"link\":\"/chatai/22\"},{\"text\":\"chatai23\",\"link\":\"/chatai/23\"},{\"text\":\"chatai24\",\"link\":\"/chatai/24\"},{\"text\":\"chatai25\",\"link\":\"/chatai/25\"},{\"text\":\"chatai26\",\"link\":\"/chatai/26\"},{\"text\":\"chatai27\",\"link\":\"/chatai/27\"},{\"text\":\"chatai28\",\"link\":\"/chatai/28\"},{\"text\":\"chatai29\",\"link\":\"/chatai/29\"},{\"text\":\"chatai30\",\"link\":\"/chatai/30\"},{\"text\":\"chatai31\",\"link\":\"/chatai/31\"},{\"text\":\"chatai32\",\"link\":\"/chatai/32\"},{\"text\":\"chatai33\",\"link\":\"/chatai/33\"},{\"text\":\"chatai34\",\"link\":\"/chatai/34\"},{\"text\":\"chatai35\",\"link\":\"/chatai/35\"},{\"text\":\"chatai36\",\"link\":\"/chatai/36\"},{\"text\":\"chatai37\",\"link\":\"/chatai/37\"},{\"text\":\"chatai38\",\"link\":\"/chatai/38\"},{\"text\":\"chatai39\",\"link\":\"/chatai/39\"},{\"text\":\"chatai40\",\"link\":\"/chatai/40\"},{\"text\":\"chatai41\",\"link\":\"/chatai/41\"},{\"text\":\"chatai42\",\"link\":\"/chatai/42\"},{\"text\":\"chatai43\",\"link\":\"/chatai/43\"},{\"text\":\"chatai44\",\"link\":\"/chatai/44\"},{\"text\":\"chatai45\",\"link\":\"/chatai/45\"},{\"text\":\"chatai46\",\"link\":\"/chatai/46\"},{\"text\":\"chatai47\",\"link\":\"/chatai/47\"},{\"text\":\"chatai48\",\"link\":\"/chatai/48\"},{\"text\":\"chatai49\",\"link\":\"/chatai/49\"},{\"text\":\"chatai50\",\"link\":\"/chatai/50\"},{\"text\":\"chatai51\",\"link\":\"/chatai/51\"},{\"text\":\"chatai52\",\"link\":\"/chatai/52\"},{\"text\":\"chatai53\",\"link\":\"/chatai/53\"},{\"text\":\"chatai54\",\"link\":\"/chatai/54\"},{\"text\":\"chatai55\",\"link\":\"/chatai/55\"},{\"text\":\"chatai56\",\"link\":\"/chatai/56\"},{\"text\":\"chatai57\",\"link\":\"/chatai/57\"},{\"text\":\"chatai58\",\"link\":\"/chatai/58\"},{\"text\":\"chatai59\",\"link\":\"/chatai/59\"},{\"text\":\"chatai60\",\"link\":\"/chatai/60\"},{\"text\":\"chatai61\",\"link\":\"/chatai/61\"},{\"text\":\"chatai62\",\"link\":\"/chatai/62\"},{\"text\":\"chatai63\",\"link\":\"/chatai/63\"},{\"text\":\"chatai64\",\"link\":\"/chatai/64\"},{\"text\":\"chatai65\",\"link\":\"/chatai/65\"},{\"text\":\"chatai66\",\"link\":\"/chatai/66\"},{\"text\":\"chatai67\",\"link\":\"/chatai/67\"},{\"text\":\"chatai68\",\"link\":\"/chatai/68\"}]},{\"text\":\"library\",\"activeMatch\":\"^/library/\",\"items\":[{\"text\":\"library1\",\"link\":\"/library/1\"},{\"text\":\"library2\",\"link\":\"/library/2\"},{\"text\":\"library3\",\"link\":\"/library/3\"},{\"text\":\"library4\",\"link\":\"/library/4\"},{\"text\":\"library5\",\"link\":\"/library/5\"},{\"text\":\"library6\",\"link\":\"/library/6\"},{\"text\":\"library7\",\"link\":\"/library/7\"},{\"text\":\"library8\",\"link\":\"/library/8\"},{\"text\":\"library9\",\"link\":\"/library/9\"},{\"text\":\"library10\",\"link\":\"/library/10\"},{\"text\":\"library11\",\"link\":\"/library/11\"},{\"text\":\"library12\",\"link\":\"/library/12\"},{\"text\":\"library13\",\"link\":\"/library/13\"},{\"text\":\"library14\",\"link\":\"/library/14\"},{\"text\":\"library15\",\"link\":\"/library/15\"},{\"text\":\"library16\",\"link\":\"/library/16\"},{\"text\":\"library17\",\"link\":\"/library/17\"},{\"text\":\"library18\",\"link\":\"/library/18\"},{\"text\":\"library19\",\"link\":\"/library/19\"},{\"text\":\"library20\",\"link\":\"/library/20\"},{\"text\":\"library21\",\"link\":\"/library/21\"},{\"text\":\"library22\",\"link\":\"/library/22\"},{\"text\":\"library23\",\"link\":\"/library/23\"},{\"text\":\"library24\",\"link\":\"/library/24\"},{\"text\":\"library25\",\"link\":\"/library/25\"},{\"text\":\"library26\",\"link\":\"/library/26\"},{\"text\":\"library27\",\"link\":\"/library/27\"},{\"text\":\"library28\",\"link\":\"/library/28\"},{\"text\":\"library29\",\"link\":\"/library/29\"},{\"text\":\"library30\",\"link\":\"/library/30\"},{\"text\":\"library31\",\"link\":\"/library/31\"},{\"text\":\"library32\",\"link\":\"/library/32\"},{\"text\":\"library33\",\"link\":\"/library/33\"},{\"text\":\"library34\",\"link\":\"/library/34\"},{\"text\":\"library35\",\"link\":\"/library/35\"},{\"text\":\"library36\",\"link\":\"/library/36\"},{\"text\":\"library37\",\"link\":\"/library/37\"},{\"text\":\"library38\",\"link\":\"/library/38\"},{\"text\":\"library39\",\"link\":\"/library/39\"},{\"text\":\"library40\",\"link\":\"/library/40\"},{\"text\":\"library41\",\"link\":\"/library/41\"},{\"text\":\"library42\",\"link\":\"/library/42\"},{\"text\":\"library43\",\"link\":\"/library/43\"},{\"text\":\"library44\",\"link\":\"/library/44\"},{\"text\":\"library45\",\"link\":\"/library/45\"},{\"text\":\"library46\",\"link\":\"/library/46\"},{\"text\":\"library47\",\"link\":\"/library/47\"},{\"text\":\"library48\",\"link\":\"/library/48\"},{\"text\":\"library49\",\"link\":\"/library/49\"},{\"text\":\"library50\",\"link\":\"/library/50\"},{\"text\":\"library51\",\"link\":\"/library/51\"},{\"text\":\"library52\",\"link\":\"/library/52\"},{\"text\":\"library53\",\"link\":\"/library/53\"},{\"text\":\"library54\",\"link\":\"/library/54\"},{\"text\":\"library55\",\"link\":\"/library/55\"},{\"text\":\"library56\",\"link\":\"/library/56\"},{\"text\":\"library57\",\"link\":\"/library/57\"},{\"text\":\"library58\",\"link\":\"/library/58\"},{\"text\":\"library59\",\"link\":\"/library/59\"},{\"text\":\"library60\",\"link\":\"/library/60\"},{\"text\":\"library61\",\"link\":\"/library/61\"},{\"text\":\"library62\",\"link\":\"/library/62\"},{\"text\":\"library63\",\"link\":\"/library/63\"},{\"text\":\"library64\",\"link\":\"/library/64\"},{\"text\":\"library65\",\"link\":\"/library/65\"},{\"text\":\"library66\",\"link\":\"/library/66\"},{\"text\":\"library67\",\"link\":\"/library/67\"},{\"text\":\"library68\",\"link\":\"/library/68\"}]},{\"text\":\"ecosystem\",\"activeMatch\":\"^/ecosystem/\",\"items\":[{\"text\":\"website\",\"items\":[{\"text\":\"partners\",\"link\":\"/partners/\"},{\"text\":\"website\",\"link\":\"/ecosystem/themes\"},{\"text\":\"deepseekletters\",\"link\":\"/ecosystem/newsletters\"},{\"text\":\"AI Navigation\",\"link\":\"/ecosystem/navigation\"},{\"text\":\"DeepSeek-V3\",\"link\":\"/ecosystem/DeepSeek\"},{\"text\":\"474x.com\",\"link\":\"https://www.474x.com\"},{\"text\":\"494x.com\",\"link\":\"https://www.494x.com\"},{\"text\":\"64ii.com\",\"link\":\"https://www.64ii.com\"},{\"text\":\"81oo.com\",\"link\":\"https://www.81oo.com\"}]},{\"text\":\"Library\",\"items\":[{\"text\":\"Vue Router\",\"link\":\"https://e.m44m.com/\"},{\"text\":\"Pinia\",\"link\":\"https://f.m44m.com/\"},{\"text\":\"tool\",\"link\":\"https://www.82ii.com\"}]},{\"text\":\"Vue\",\"items\":[{\"text\":\"Vue Mastery\",\"link\":\"https://g.m44m.com\"},{\"text\":\"Vue School\",\"link\":\"https://h.m44m.com\"}]},{\"text\":\"help\",\"items\":[{\"text\":\"Discord\",\"link\":\"https://i.m44m.com\"},{\"text\":\"GitHub\",\"link\":\"https://github.com/hyaliyun/MIT\"},{\"text\":\"DEV\",\"link\":\"https://www.z2.pw\"}]},{\"text\":\"MIT\",\"items\":[{\"text\":\"blog\",\"link\":\"https://c.m44m.com\"},{\"text\":\"Twitter\",\"link\":\"https://d.m44m.com\"},{\"text\":\"Activity\",\"link\":\"https://e.m44m.com\"},{\"text\":\"CMS\",\"link\":\"https://w.z2.pw\"},{\"text\":\"deepseekmagSheets\",\"link\":\"https://a.z2.pw\"},{\"text\":\"Tailwind\",\"link\":\"https://a.434x.com\"},{\"text\":\"Three.js\",\"link\":\"https://b.434x.com\"},{\"text\":\"youtube\",\"link\":\"https://www.q8q9.com\"}]}]},{\"text\":\"team\",\"link\":\"/about/team\",\"activeMatch\":\"^/about/\"},{\"text\":\"show\",\"activeMatch\":\"^/(guide|style-guide|cookbook|examples)/\",\"items\":[{\"text\":\"donation\",\"link\":\"/drive/donation\"},{\"text\":\"PromptLibrary\",\"link\":\"/drive/PromptLibrary\"},{\"text\":\"prompt\",\"link\":\"/drive/prompt\"},{\"text\":\"crypto\",\"link\":\"/drive/team\"},{\"text\":\"partners\",\"link\":\"/partners/\"},{\"text\":\"3kk3.com\",\"link\":\"https://www.3kk3.com\"},{\"text\":\"deepseek\",\"link\":\"https://b.q8q9.com\"},{\"text\":\"deepseekr1\",\"link\":\"https://c.4s5s.com\"},{\"text\":\"deepseekr2\",\"link\":\"https://b.6n7n.com\"},{\"text\":\"deepseekr3\",\"link\":\"https://f.m44m.com\"},{\"text\":\"deepseekr4\",\"link\":\"https://c.q8q9.com\"},{\"text\":\"deepseekr5\",\"link\":\"https://a.l00m.com\"},{\"text\":\"deepseekr6\",\"link\":\"https://g.m44m.com\"}]},{\"text\":\"swap\",\"link\":\"/swap/app\",\"activeMatch\":\"^/swap/\"}],\"sidebar\":{},\"localeLinks\":[{\"link\":\"https://g.m44m.com\",\"text\":\"\",\"repo\":\"https://github.com/hyaliyun/MIT\"}],\"algolia\":{\"indexName\":\"MIT\",\"appId\":\"ML0LEBN7FQ\",\"Key\":\"21cf9df0734770a2448a9da64a700c22\",\"searchParameters\":{\"facetFilters\":[\"version:v3\"]}},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/hyaliyun/MIT/\"}],\"editLink\":{\"repo\":\"hyaliyun/MIT\",\"text\":\"Edit this page on GitHub\"},\"footer\":{\"license\":{\"text\":\"MIT License\",\"link\":\"https://www.m44m.com\"},\"copyright\":\"Copyright  2014-2025 MIT\"}},\"locales\":{},\"scrollOffset\":[\"header\",\".VPLocalNav\"],\"cleanUrls\":false}");</script>
    
  </body>
</html>